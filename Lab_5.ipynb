{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-qc/lab5/blob/main/Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlJoxmNsB9Ql"
      },
      "source": [
        "# Lab 5. - Introduction to Quantum Machine Learning (SVMs, CNNs)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXzdjlCGDNfY"
      },
      "source": [
        "**Machine learning** has established itself as a successful interdisciplinary field which seeks to find patterns in data. Throwing in quantum computing gives rise to interesting areas of research that aim to use the principles of quantum mechanics to augment machine learning or vice-versa. On this page, we aim to give you a glimpse into the exciting and rapidly changing field of quantum machine learning.\n",
        "\n",
        "Before we dive into quantum machine learning, let's do a whirlwind overview of machine learning. For our purposes, machine learning can be splited very roughly into two subfields: supervised learning, and unsupervised learning.\n",
        "\n",
        "**Supervised learning** is a type of machine learning where a model is trained on a labeled dataset, meaning each training example is paired with an output label. The model learns to map inputs to the correct output and is then used to predict labels for new, unseen data.\n",
        "\n",
        "![screenshot_1747215435312.png](<https://media-hosting.imagekit.io/650ed97b9bff42f9/screenshot_1747215435312.png?Expires=1841823435&Key-Pair-Id=K2ZIVPTIP2VGHC&Signature=M0Ddnz3Vql0gDvlVtxoQ9bcIYmdHy-ze-Of3ov-uwcOsfYUoyZZZtZsWRGb5wA8DzlbUQr3vB3bJYQyqPjfsYQw4VurZ3itiWNu6L4-TzjV6P~c77Xu7HGe7SoNcu59OL7aE9TpalVJrrVUouM8OarOXG2LQLqYCwnDsIvZXJLwTrYLzuycor3zsc7bUXDLvnmctSdFENuZg9XSJLJ3cQUIDnwcJ24QiwInRvNWosCFTlreZ8C~gOvZkyPBgt7Thu4xumF8H6oPJ9YUQhNJR2tE36fawGlDKvArRF7m6qppNjetbDKhHS-z5szqoxgXGBAaB8fsWz3kfDcEabtSM8A__>)\n",
        "\n",
        "Therefore, given tuples of labeled data $(x_i,y_i)$, we aim to learn the function that maps $f: x \\mapsto y$ and generalizes to unseen inputs. For example, given a set of labeled photos of cats or dogs, we want to identify new photos of cats or dogs.\n",
        "\n",
        "\n",
        "**Unsupervised learning** is a type of machine learning where the model is trained on data without labeled responses. It aims to discover hidden patterns, structures, or groupings within the input data, such as through clustering or dimensionality reduction.\n",
        "\n",
        "![screenshot_1747215482970.png](<https://media-hosting.imagekit.io/3e0166e71d62474b/screenshot_1747215482970.png?Expires=1841823482&Key-Pair-Id=K2ZIVPTIP2VGHC&Signature=c8VaevzvdMnCK6nTIZhqHqUfLNAut4LUzyXpQk9i8AvXW1FeZuHEV-8Y6F1B64UyjY3hLaXEjYhYUF6gugCwJhOg-LX7ytdWvr7hWV1q9b1AeEV-xRpKXxD5pYVvqDfU0WXvHWkCGm~8HUwThwEoCYHE~sYqZkyb-D~7yEXkpDpTBmOlSBlxFj8mBvMNPKAEsgjaamvgfeafbp4D7l2EF7r1vJ-P-t-ZVJ-WwO~ooKg-LvmEmVEPPPsyn28v49OKQTYX1Ut-EHKouJbZVz1oeHLkjXDoc2UV3puQIG2Nu8Xg8ur6vVN8-BOBLZkm-3~PX7gayLaGunQ9zB~3sGeNyA__>)\n",
        "\n",
        "Therefore, given a collection of unlabeled data $(x_i)$, we aim to learn some structure of the data; for example, grouping a set of viewers based on their movie viewing history in order to recommend new movies.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUpQDuA7Lbza"
      },
      "source": [
        "## Quantum Machine Learning (QML)\n",
        "\n",
        "There are four different approaches to combining quantum computing and machine learning, differentiated by whether the data is classical **C** or quantum **Q**, or whether the algorithm runs on a classical **C** or quantum **Q** computer as shown in figure below.\n",
        "\n",
        "![screenshot_1747216404764.png](<https://media-hosting.imagekit.io/8f7acc9b70aa4409/screenshot_1747216404764.png?Expires=1841824404&Key-Pair-Id=K2ZIVPTIP2VGHC&Signature=2ooS4Vb2OOPpTJsjfpC~GIg3rz~Q-5sCpBmQJjXM1Q3lv4GRKyxs7EpbY83sQ3N5kxiaehxCekAay1gBICeaN6vjbNujQlKcg2V1uycnI1RSEhxTBGmzPDNmlPvJwouV-g4l5QWo0TC8ep5ryzZuppcCSJAgSezeyVtfi0CohBwbwwXNZStE~OmDuFzUmxAJTfX~6lFR0aa4sb6tPQsuiTZiHgE~kbAGeSxpyU1wjgOYmvfrZCtNUN5GJamlRotME-UagaaRix5xkFZi0BGQvvmILKCV-BcWpRqlsaIFwhug5PNWCSVdX2ffq~~X6ueKwG-LebE78JSb5u0YOw5pQw__>)\n",
        "\n",
        "In this context, a quantum dataset consists of observations from a natural or artificial quantum system, such as measurements of qubit interactions, while a classical dataset consists of observations from a classical system, such as time series, text or images.\n",
        "\n",
        "On the Figure above:\n",
        "\n",
        "1. **CC** refers to processing Classical data using Classical computers, but using algorithms inspired by quantum computing.\n",
        "\n",
        "2. **CQ** refers to processing Classical data using Quantum machine learning algorithms and will be what this chapter focuses on.\n",
        "\n",
        "3. **QC** refers to processing Quantum data using Classical machine learning algorithms.\n",
        "\n",
        "4. **QQ** refers to processing Quantum data using Quantum machine learning algorithms. This is an interesting topic, but very much still in its infancy.\n",
        "\n",
        "There are two distinct categories of **QC** algorithms: those that require quantum random access memory qRAM, where data can be accessed in superposition, and those that don't. The various proposed qRAM-based QML algorithms, e.g. [qPCA](https://doi.org/10.1038/nphys3029), [qSVM](https://doi.org/10.1103/PhysRevLett.113.130503) and [qClustering](https://arxiv.org/abs/1307.0411), boast exponential speedups compared to their classical algorithms, however there are currently no viable hardware candidates for realizing qRAM.\n",
        "\n",
        "Recently, most of the focus of **CQ** approaches to machine learning has been on near-term algorithms that can be executed on the current quantum devices. Classical machine learning techniques have made great strides in the past decade, enabled in large part by the availability of sufficiently powerful hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQpLxHN6915q"
      },
      "source": [
        "First, let's install latest version of `qiskit-machine-learning` package and `qiskit`. Currently, latest `qiskit-machine-learning` package works only with `qiskit` version 1.1.0 (although newier versions of quiskit are available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Elkz8y-ahy-i"
      },
      "outputs": [],
      "source": [
        "# Install exactly the working versions\n",
        "!pip install --no-cache-dir \"qiskit==1.1.0\" \"qiskit-machine-learning==0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAyEIgout79e"
      },
      "outputs": [],
      "source": [
        "# Check if primitives are workin, if this runs smoothly, we are good.\n",
        "from qiskit.primitives import Sampler\n",
        "print(\"Sampler is now available!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDetgVytWnM0"
      },
      "source": [
        "##  1. Kernel Methods for Machine Learning\n",
        "\n",
        "Kernel methods are a collection of pattern analysis algorithms that use kernel functions to operate in a high-dimensional feature space. The best-known application of kernel methods is in **Support Vector Machines (SVMs)**, supervised learning algorithms commonly used for classification tasks.\n",
        "\n",
        "The main goal of SVMs is to find decision boundaries to separate a given set of data points into classes. When these data spaces are not linearly separable, SVMs can benefit from the use of kernels to find these boundaries.\n",
        "\n",
        "Formally, decision boundaries are hyperplanes in a high dimensional space. The kernel function implicitly maps input data into this higher dimensional space, where it can be easier to solve the initial problem. In other words, kernels may allow data distributions that were originally non-linearly separable to become a linearly separable problem. This is an effect known as the \"kernel trick\".\n",
        "\n",
        "![screenshot_1747228307639.png](<https://media-hosting.imagekit.io/e18637519e724420/screenshot_1747228307639.png?Expires=1841836308&Key-Pair-Id=K2ZIVPTIP2VGHC&Signature=YLA8I2dwvjtMeOR29Mdn7L2WqYlZDE3wscwrgHWTBsQf83kF5pGfMkBdrk271CgHTwaddo0EE2vLPlSgkieTS0uL72v5M~-gRP~rsUleGdQIMctWstrfle98Ca-qyeuDRLWqUqW7~uKf1Twn0q0Rh1c6vM5QAvfx8Sf0scpUkksXOAToT52OrqzYsC40nfDfkQlKam1~DNVmeMNGiKoKcdf8egT3dm07Chi6ugSyWuXJRxJWQoTEs8sF-Rq5PUTSOyKQ11yNjg6HRmw3TWXaF-F3OjlGyMywY14m-AgDZtyf-dB8HEbuZA3LKyuwdW2WQgoYs~IlsHLguqBhmHV-Hw__>)\n",
        "\n",
        "There are use-cases for kernel-based unsupervised algorithms too, for example, in the context of clustering.\n",
        "\n",
        "**Spectral Clustering** is a technique where data points are treated as nodes of a graph, and the clustering task is viewed as a graph partitioning problem where nodes are mapped to a space where they can be easily segregated to form clusters.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1. Kernel Functions\n",
        "\n",
        "Mathematically, kernel functions follow:\n",
        "\n",
        "$k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle$\n",
        "\n",
        "where:\n",
        "\n",
        "* $k$ is the kernel function\n",
        "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs\n",
        "* $f$ is a map from $n$-dimension to $m$-dimension space and\n",
        "* $\\langle a,b \\rangle$ denotes the inner product\n",
        "\n",
        "When considering finite data, a kernel function can be represented as a matrix:\n",
        "\n",
        "$K_{ij} = k(\\vec{x}_i,\\vec{x}_j)$.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2. Quantum Kernels\n",
        "\n",
        "The main idea behind quantum kernel machine learning is to leverage quantum feature maps to perform the kernel trick. In this case, the quantum kernel is created by mapping a classical feature vector $\\vec{x}$ to a Hilbert space using a quantum feature map $\\phi(\\vec{x})$. Mathematically, this is written as:\n",
        "\n",
        "$K_{ij} = \\left| \\langle \\phi(\\vec{x}_i)| \\phi(\\vec{x}_j) \\rangle \\right|^{2}$\n",
        "\n",
        "where:\n",
        "\n",
        "* $K_{ij}$ is the kernel matrix\n",
        "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs\n",
        "* $\\phi(\\vec{x})$ is the quantum feature map\n",
        "* $\\left| \\langle a|b \\rangle \\right|^{2}$ denotes the overlap of two quantum states $a$ and $b$.\n",
        "\n",
        "Quantum kernels can be plugged into common classical kernel learning algorithms such as SVMs or clustering algorithms, as you will see in the examples below. They can also be leveraged in new quantum kernel methods like QSVC class provided by `qiskit-machine-learning` which is explored in this tutorial, and other methods as shown in later tutorials on Pegasos QSVC and Quantum Kernel Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bRuHFMsSsbEr"
      },
      "outputs": [],
      "source": [
        "# Import the global configuration utility from Qiskit Machine Learning.\n",
        "# This allows setting global parameters such as the random seed for reproducibility.\n",
        "from qiskit_machine_learning.utils import algorithm_globals\n",
        "\n",
        "# Set the global random seed to ensure reproducible results across runs.\n",
        "# This affects all randomized components within Qiskit Machine Learning (e.g., data generation, model initialization).\n",
        "algorithm_globals.random_seed = 12345"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzaWIFWMsbEr"
      },
      "source": [
        "###  1.2. Quantum Kernel-Based Classification\n",
        "\n",
        "Now we will demonstrate the process of performing quantum kernel-based classification using the `qiskit-machine-learning` library.As already mentioned, kernel methods are particularly useful in machine learning for capturing complex patterns in data by implicitly mapping input features into higher-dimensional spaces. Quantum kernels leverage quantum circuits to compute these mappings, potentially providing advantages over classical kernels in certain problem domains.\n",
        "\n",
        "We begin by defining the dataset to be used for training and evaluating our quantum kernel classifier. For this example, we use the _ad hoc dataset_, a synthetic binary classification dataset specifically designed to highlight potential quantum advantages in kernel-based learning as described in the reference [paper](https://arxiv.org/pdf/1804.11326.pdf).\n",
        "\n",
        "The ad_hoc_data function from qiskit_machine_learning.datasets allows us to generate training and test subsets with customizable properties. Specifically, we can control:\n",
        "- the dimensionality of the feature space (i.e., number of qubits),\n",
        "- the number of samples per class for both training and testing,\n",
        "- the margin (or \"gap\") between the classes, which affects classification difficulty,\n",
        "- and whether or not we return the total number of samples and one-hot encoded labels.\n",
        "\n",
        "In the code below, we set the dimension to 2, specify 20 training samples and 5 test samples per class, and generate the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VLiACVTGsbEs"
      },
      "outputs": [],
      "source": [
        "# Import the ad_hoc_data dataset generator from Qiskit Machine Learning.\n",
        "# This function creates a synthetic classification dataset designed for benchmarking quantum kernels.\n",
        "from qiskit_machine_learning.datasets import ad_hoc_data\n",
        "\n",
        "# Set the dimension of the input features (number of qubits for encoding).\n",
        "adhoc_dimension = 2\n",
        "\n",
        "# Generate training and test data using the ad_hoc_data function.\n",
        "# - training_size: number of training samples per class\n",
        "# - test_size: number of test samples per class\n",
        "# - n: feature dimension (number of qubits/features)\n",
        "# - gap: separation between the two classes (larger = easier to separate)\n",
        "# - plot_data: if True, will plot the generated data\n",
        "# - one_hot: if True, labels are one-hot encoded\n",
        "# - include_sample_total: if True, returns the total number of generated samples\n",
        "\n",
        "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
        "    training_size=20,\n",
        "    test_size=5,\n",
        "    n=adhoc_dimension,\n",
        "    gap=0.3,\n",
        "    plot_data=False,\n",
        "    one_hot=False,\n",
        "    include_sample_total=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWcdenvlsbEs"
      },
      "source": [
        "This dataset is two-dimensional, the two features are represented by the $x$ and $y$ coordinates, and it has two class labels: A and B. We can plot it and see what the distribution looks like. We define utility functions to plot the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hJCx9IalsbEs"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for plotting and numerical operations\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a helper function to plot individual feature groups\n",
        "def plot_features(ax, features, labels, class_label, marker, face, edge, label):\n",
        "    # Scatter plot points belonging to a specific class (class_label)\n",
        "    ax.scatter(\n",
        "        # x-coordinates of samples where label matches class_label\n",
        "        features[np.where(labels[:] == class_label), 0],\n",
        "        # y-coordinates of samples where label matches class_label\n",
        "        features[np.where(labels[:] == class_label), 1],\n",
        "        marker=marker,       # Shape of the marker (e.g., square or circle)\n",
        "        facecolors=face,     # Fill color of the marker\n",
        "        edgecolors=edge,     # Edge color of the marker\n",
        "        label=label          # Label for legend\n",
        "    )\n",
        "\n",
        "# Define a function to visualize the entire dataset (training + testing)\n",
        "def plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total):\n",
        "\n",
        "    # Create a new figure with fixed size\n",
        "    plt.figure(figsize=(5, 5))\n",
        "\n",
        "    # Set axis limits from 0 to 2π to match the ad hoc feature space\n",
        "    plt.ylim(0, 2 * np.pi)\n",
        "    plt.xlim(0, 2 * np.pi)\n",
        "\n",
        "    # Plot the background color map based on the full feature space\n",
        "    plt.imshow(\n",
        "        np.asmatrix(adhoc_total).T,    # Transpose to match axis orientation\n",
        "        interpolation=\"nearest\",       # Nearest-neighbor interpolation\n",
        "        origin=\"lower\",                # Origin at bottom-left\n",
        "        cmap=\"RdBu\",                   # Red-blue colormap for class separation\n",
        "        extent=[0, 2 * np.pi, 0, 2 * np.pi],  # Match plot limits\n",
        "    )\n",
        "\n",
        "    # Plot training samples for class A (label 0) as white squares with blue edges\n",
        "    plot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
        "\n",
        "    # Plot training samples for class B (label 1) as white circles with red edges\n",
        "    plot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
        "\n",
        "    # Plot test samples for class A as blue-filled squares with white edges\n",
        "    plot_features(plt, test_features, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
        "\n",
        "    # Plot test samples for class B as red-filled circles with white edges\n",
        "    plot_features(plt, test_features, test_labels, 1, \"o\", \"r\", \"w\", \"B test\")\n",
        "\n",
        "    # Display the legend outside the upper right of the plot\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
        "\n",
        "    # Add a title to the plot\n",
        "    plt.title(\"Ad hoc dataset\")\n",
        "\n",
        "    # Render the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srWfIaRtsbEs"
      },
      "source": [
        "Now we actually plot the dataset for classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ePdpoiCsbEs",
        "tags": [
          "nbsphinx-thumbnail"
        ]
      },
      "outputs": [],
      "source": [
        "# Call the plot_dataset function to visualize the ad hoc dataset.\n",
        "# This function plots:\n",
        "# - The background decision surface (adhoc_total),\n",
        "# - Training samples for both classes A and B,\n",
        "# - Test samples for both classes A and B,\n",
        "# using different markers and colors.\n",
        "# It helps visually inspect class distribution and separation in feature space.\n",
        "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHJKFDtJsbEs"
      },
      "source": [
        "### 1.3. Defining the quantum kernel\n",
        "\n",
        "After generating and visualizing the dataset, the next crucial step is to define the quantum kernel, which serves as the foundation for the quantum-enhanced machine learning model. A kernel is a function that measures similarity between pairs of data points. In classical machine learning, kernel methods (such as in Support Vector Machines) are widely used to handle non-linearly separable data by implicitly mapping it to a higher-dimensional space. In this quantum variant, the similarity between data points is evaluated based on the fidelity between quantum states.\n",
        "\n",
        "To achieve this, we use the `FidelityQuantumKernel` class from Qiskit Machine Learning. This class constructs a kernel matrix using quantum circuits by computing fidelities between the quantum states that result from applying a feature map to the input data. It expects two primary components as input arguments to its constructor:\n",
        "\n",
        "1. `feature_map`: is a quantum circuit that encodes classical input features into quantum states. In our case, we use the `ZZFeatureMap`, which is commonly used in kernel-based quantum models. It works by applying parameterized single-qubit rotations followed by entangling gates (ZZ interactions) in a linear pattern. This encoding allows the quantum device to capture feature correlations in a non-trivial way. We configure it to use:\n",
        "  - 2 qubits (corresponding to the number of features),\n",
        "  - 2 repetitions of the encoding layers,\n",
        "  - Linear entanglement connecting qubits in a chain-like structure.\n",
        "\n",
        "2. `fidelity`: defines how the similarity (or inner product) between quantum states is computed. Here, we use the `ComputeUncompute` method, which estimates fidelity by running two quantum circuits - one for each input - and comparing their final states. Internally, it uses the `Sampler` primitive to execute the circuits and collect measurement outcomes. The `Sampler` returns ideal or noisy samples depending on the backend and configuration used.\n",
        "\n",
        "We explicitly instantiate the `Sampler` and pass it to `ComputeUncompute`, which is then passed to the `FidelityQuantumKernel`. This setup ensures deterministic, reproducible results when using simulators and provides flexibility to extend the model to real quantum hardware.\n",
        "\n",
        "**NOTE:** If you don't pass a `Sampler` or `Fidelity` instance, then the instances of the reference `Sampler` and `ComputeUncompute` classes (found in `qiskit.primitives`) will be created by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eko5w_QGsbEt"
      },
      "outputs": [],
      "source": [
        "# Import the ZZFeatureMap, a parameterized quantum feature map that encodes classical data into quantum states\n",
        "# using ZZ entangling interactions. Commonly used in quantum kernel methods.\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "\n",
        "# Import the StatevectorSampler and alias it as \"Sampler\".\n",
        "# This sampler simulates quantum circuits and returns ideal statevectors (no noise).\n",
        "from qiskit.primitives import StatevectorSampler as Sampler\n",
        "\n",
        "# Import the ComputeUncompute fidelity estimator.\n",
        "# This component estimates the fidelity (similarity) between two quantum states using the \"compute-uncompute\" method.\n",
        "from qiskit_machine_learning.state_fidelities import ComputeUncompute\n",
        "\n",
        "# Import the FidelityQuantumKernel class, which uses quantum state fidelity to compute kernel matrices.\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "\n",
        "# Create a ZZFeatureMap circuit with the same dimension as the dataset.\n",
        "# - feature_dimension: number of qubits (equal to number of input features)\n",
        "# - reps: number of times to repeat the entangling layers\n",
        "# - entanglement: entangling strategy (\"linear\" connects qubits in a chain)\n",
        "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
        "\n",
        "# Initialize a statevector-based sampler that computes the ideal output state of quantum circuits.\n",
        "sampler = Sampler()\n",
        "\n",
        "# Instantiate a ComputeUncompute fidelity estimator using the defined sampler.\n",
        "fidelity = ComputeUncompute(sampler=sampler)\n",
        "\n",
        "# Construct a fidelity-based quantum kernel using the feature map and fidelity estimator.\n",
        "# This kernel can later be used in quantum-enhanced classification tasks.\n",
        "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5NB738DsbEt"
      },
      "source": [
        "### 1.4. Classification with SVC\n",
        "\n",
        "With the quantum kernel (`FidelityQuantumKernel`) defined, we can now integrate it into a classical machine learning pipeline to perform classification. A natural choice for this task is the Support Vector Classifier (SVC) provided by the popular `scikit-learn` library. SVC is a supervised learning algorithm that constructs a decision boundary (hyperplane) that maximally separates different classes in a transformed feature space - a space defined by the kernel function.\n",
        "\n",
        "In the context of quantum machine learning, the key idea is to replace the classical kernel (e.g., linear, polynomial or RBF) with a quantum kernel, which captures the similarity between data points via quantum circuits. This can potentially allow the classifier to capture more complex relationships in data that are difficult for classical kernels to model.\n",
        "\n",
        "---\n",
        "\n",
        "There are two ways to use custom kernels with SVC:\n",
        "\n",
        "1. **Using a callable kernel function (on-the-fly evaluation)**\n",
        "\n",
        "You can pass a function to the `kernel` parameter that accepts two arguments: the feature matrices of the training and test datasets. This function is expected to return the kernel (Gram) matrix - a square matrix where each entry `[i, j]` represents the kernel similarity between the `i`-th and `j`-th sample.\n",
        "\n",
        "In our case, we pass the `.evaluate` method of the `FidelityQuantumKernel` object, which internally computes the kernel matrix using the quantum feature map and fidelity estimator:\n",
        "\n",
        "`adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)`\n",
        "\n",
        "This approach computes the kernel values during training and inference, which offers flexibility (e.g., using different samplers, real quantum backends), but may increase computation time.\n",
        "\n",
        "---\n",
        "\n",
        "2.**Using a precomputed kernel matrix**\n",
        "\n",
        "Alternatively, the kernel matrix can be precomputed for both training and test sets. In this approach, you:\n",
        "- Compute the training kernel matrix `K_train = kernel.evaluate(X_train)`\n",
        "- Train the SVC with `kernel='precomputed'` and pass `K_train` as input\n",
        "- Compute the test kernel matrix `K_test = kernel.evaluate(X_test, X_train)`\n",
        "- Evaluate the model on `K_test`\n",
        "\n",
        "This is useful when the kernel matrix is expensive to compute (e.g., on quantum hardware) and you want to avoid recomputation.\n",
        "\n",
        "In our example, we follow the **callable function** approach for simplicity and flexibility. The trained quantum SVC is then evaluated using the `.score()` method, which returns the classification accuracy on the test set.\n",
        "\n",
        "This demonstrates how quantum components (kernel matrix via quantum fidelity) can be seamlessly integrated into classical machine learning workflows, enabling hybrid quantum-classical models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edwZDbvuF-u3"
      },
      "source": [
        "#### 1.4.1. Kernel as a callable function\n",
        "\n",
        "We define a SVC model and directly pass the `evaluate` function of the quantum kernel as a callable. Once the model is created, we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4_w2d_xsbEt"
      },
      "outputs": [],
      "source": [
        "# Import the Support Vector Classifier (SVC) from scikit-learn.\n",
        "# This classifier supports custom kernels, including quantum kernels.\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create an instance of SVC using a custom kernel function.\n",
        "# We pass the 'evaluate' method of the FidelityQuantumKernel object,\n",
        "# which computes the quantum kernel matrix between input samples.\n",
        "adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)\n",
        "\n",
        "# Train the quantum kernel-based SVC using the training dataset.\n",
        "# The evaluate() method will be used internally to compute the kernel matrix.\n",
        "adhoc_svc.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate the trained classifier on the test dataset using the score() method.\n",
        "# This returns the classification accuracy (correct predictions / total samples).\n",
        "adhoc_score_callable_function = adhoc_svc.score(test_features, test_labels)\n",
        "\n",
        "# Print the accuracy score of the quantum kernel-based classifier.\n",
        "print(f\"Callable kernel classification test score: {adhoc_score_callable_function}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t_oGtrrsbEt"
      },
      "source": [
        "#### 1.4.2. Precomputed kernel matrix\n",
        "\n",
        "Instead of passing a function of the quantum kernel as a callable, we can also precompute training and testing kernel matrices before passing them to the `scikit-learn` `SVC` algorithm.\n",
        "\n",
        "To extract the train and test matrices, we can call `evaluate` on the previously defined kernel and visualize them graphically as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y2FWWMlsbEt",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Compute the quantum kernel matrix for the training dataset.\n",
        "# This evaluates the similarity between all pairs of training samples using the quantum kernel.\n",
        "adhoc_matrix_train = adhoc_kernel.evaluate(x_vec=train_features)\n",
        "\n",
        "# Compute the quantum kernel matrix between test and training samples.\n",
        "# This evaluates how similar each test sample is to each training sample.\n",
        "adhoc_matrix_test = adhoc_kernel.evaluate(x_vec=test_features, y_vec=train_features)\n",
        "\n",
        "# Create a side-by-side plot layout with 1 row and 2 columns.\n",
        "# The overall figure size is set to 10 inches wide by 5 inches tall.\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Visualize the training kernel matrix as an image on the first subplot.\n",
        "# Each pixel represents the quantum similarity between a pair of training samples.\n",
        "axs[0].imshow(\n",
        "    np.asmatrix(adhoc_matrix_train),     # Convert kernel matrix to a matrix type for display\n",
        "    interpolation=\"nearest\",             # Use nearest-neighbor interpolation\n",
        "    origin=\"upper\",                      # Set origin at the top-left corner\n",
        "    cmap=\"Blues\"                         # Use a blue colormap\n",
        ")\n",
        "axs[0].set_title(\"Ad hoc training kernel matrix\")  # Set the title of the first subplot\n",
        "\n",
        "# Visualize the test vs. train kernel matrix on the second subplot.\n",
        "# This shows the similarity between test samples and training samples.\n",
        "axs[1].imshow(\n",
        "    np.asmatrix(adhoc_matrix_test),\n",
        "    interpolation=\"nearest\",\n",
        "    origin=\"upper\",\n",
        "    cmap=\"Reds\"                          # Use a red colormap for contrast\n",
        ")\n",
        "axs[1].set_title(\"Ad hoc testing kernel matrix\")   # Set the title of the second subplot\n",
        "\n",
        "# Display both subplots side by side\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQTP5AaxsbEt"
      },
      "source": [
        "To use these matrices, we set the `kernel` parameter of a new `SVC` instance to `\"precomputed\"`. We train the classifier by calling `fit` with the training matrix and training dataset. Once the model is trained, we evaluate it using the test matrix on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ_7V5D-sbEt"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the Support Vector Classifier (SVC) from scikit-learn,\n",
        "# specifying that the kernel is \"precomputed\". This means we will directly pass\n",
        "# a precomputed kernel (Gram) matrix instead of raw feature vectors.\n",
        "adhoc_svc = SVC(kernel=\"precomputed\")\n",
        "\n",
        "# Train the SVC model using the precomputed training kernel matrix and the corresponding labels.\n",
        "# Each element [i, j] of the matrix represents the kernel similarity between training samples i and j.\n",
        "adhoc_svc.fit(adhoc_matrix_train, train_labels)\n",
        "\n",
        "# Evaluate the trained SVC model on the test data using the precomputed test-vs-train kernel matrix.\n",
        "# This matrix contains kernel similarities between each test sample and all training samples.\n",
        "adhoc_score_precomputed_kernel = adhoc_svc.score(adhoc_matrix_test, test_labels)\n",
        "\n",
        "# Print the accuracy score of the classifier that used the precomputed quantum kernel matrix.\n",
        "print(f\"Precomputed kernel classification test score: {adhoc_score_precomputed_kernel}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7LDdaDsbEt"
      },
      "source": [
        "#### 1.5. Classification with QSVC\n",
        "\n",
        "`QSVC` is an alternative training algorithm provided by `qiskit-machine-learning` for convenience. It is an extension of `SVC` that takes in a quantum kernel instead of the `kernel.evaluate` method shown before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ-nCe8WsbEt"
      },
      "outputs": [],
      "source": [
        "# Import the QSVC (Quantum Support Vector Classifier) class from Qiskit Machine Learning.\n",
        "# QSVC is a quantum-enhanced version of scikit-learn's SVC, designed to work with quantum kernels.\n",
        "from qiskit_machine_learning.algorithms import QSVC\n",
        "\n",
        "# Create an instance of QSVC using the previously defined quantum kernel.\n",
        "# The kernel must be an instance of FidelityQuantumKernel or a compatible interface.\n",
        "qsvc = QSVC(quantum_kernel=adhoc_kernel)\n",
        "\n",
        "# Train the QSVC model using the training data.\n",
        "# Internally, the kernel matrix is computed using the quantum kernel,\n",
        "# and then a support vector classifier is trained using that matrix.\n",
        "qsvc.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate the performance of the trained QSVC model on the test data.\n",
        "# This returns the classification accuracy: (correct predictions / total predictions).\n",
        "qsvc_score = qsvc.score(test_features, test_labels)\n",
        "\n",
        "# Print the classification accuracy of the QSVC model on the test dataset.\n",
        "print(f\"QSVC classification test score: {qsvc_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdb09oJ4sbEu"
      },
      "source": [
        "####  1.6. Evaluation of models used for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9lM-vK1sbEu"
      },
      "outputs": [],
      "source": [
        "print(f\"Classification Model                    | Accuracy Score\")\n",
        "print(f\"---------------------------------------------------------\")\n",
        "print(f\"SVC using kernel as a callable function | {adhoc_score_callable_function:10.2f}\")\n",
        "print(f\"SVC using precomputed kernel matrix     | {adhoc_score_precomputed_kernel:10.2f}\")\n",
        "print(f\"QSVC                                    | {qsvc_score:10.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVMkqZWfsbEu"
      },
      "source": [
        "As the classification dataset is small, we find that the three models achieve 100% accuracy.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI9J5l49sbEu"
      },
      "source": [
        "### 2.1. Quantum Kernel-Based Clustering\n",
        "\n",
        "The second algorithm shifts from supervised learning (classification) to unsupervised learning, specifically focusing on a clustering task. In clustering, the goal is to group unlabeled data into distinct clusters based on similarities in their features - without relying on predefined class labels.\n",
        "\n",
        "In this algorithm, we explore how quantum kernels can be used to enhance traditional clustering methods by capturing complex feature relationships in a quantum-enhanced Hilbert space.\n",
        "\n",
        "To implement this, we use tools from two main libraries:\n",
        "\n",
        "- `qiskit-machine-learning` – for defining the quantum kernel, which encodes feature similarity using quantum circuits.\n",
        "- `scikit-learn` – for applying the spectral clustering algorithm, a graph-based method that partitions data based on the eigenstructure of the kernel (similarity) matrix.\n",
        "\n",
        "##### **Dataset Creation**\n",
        "\n",
        "We define the dataset that will be used for the unsupervised learning task — specifically, clustering. Just like in the earlier classification example, we use the `ad hoc` dataset generator provided by `qiskit_machine_learning.datasets.ad_hoc_data`. This synthetic dataset is designed to highlight potential advantages of quantum kernels by encoding class structure in a way that is amenable to quantum representations.\n",
        "\n",
        "However, there is a key difference in how we configure the dataset for this clustering example. Instead of a smaller gap (gap=0.3) as used in the classification workflow, we now set a larger gap of 0.6. The gap parameter controls the separation between the two classes in the feature space:\n",
        "\n",
        "- A **smaller gap** results in more overlapping and ambiguous samples, making classification or clustering more difficult.\n",
        "- A **larger gap** makes the class structure more distinct, which can improve the performance of clustering algorithms and highlight the patterns captured by the kernel.\n",
        "\n",
        "Another important distinction is that this task falls under the umbrella of unsupervised machine learning. Unlike supervised learning — which uses labeled data for training and testing — clustering methods do not rely on labeled data during learning. The objective is to group data points into clusters based solely on their similarities or distances in the feature space.\n",
        "\n",
        "Because of this, we:\n",
        "\n",
        "- Generate only the training set (used here as the complete dataset for clustering),\n",
        "- Set the test set size to zero (test_size=0), as it is not required for unsupervised learning,\n",
        "- Still retain the original labels, which are helpful for evaluating clustering accuracy after the model has been trained — but they are not used during clustering itself.\n",
        "\n",
        "Therefore,  we define a clean, well-separated synthetic dataset that is ideal for demonstrating quantum-enhanced clustering methods. This setup allows us to explore whether quantum kernels can uncover class structure in data even when the model has no access to the actual labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qQVVQG0nsbEu"
      },
      "outputs": [],
      "source": [
        "# Set the dimensionality of the feature space to 2, which corresponds to using 2 qubits\n",
        "# in the quantum feature map. This matches the number of features per data sample.\n",
        "adhoc_dimension = 2\n",
        "\n",
        "# Generate an \"ad hoc\" synthetic quantum dataset using Qiskit's ad_hoc_data function.\n",
        "# This function returns training and testing data, labels, and the full sample space.\n",
        "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
        "    training_size=25,       # Number of training samples per class\n",
        "    test_size=0,            # No test samples will be generated (unsupervised task like clustering)\n",
        "    n=adhoc_dimension,      # Number of features (qubits), as defined above\n",
        "    gap=0.6,                # Margin (gap) between the two classes to control overlap and difficulty\n",
        "    plot_data=False,        # Don't generate a plot automatically\n",
        "    one_hot=False,          # Labels will be returned as integers (0 or 1), not one-hot vectors\n",
        "    include_sample_total=True  # Also return the complete feature space for visualization (adhoc_total)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr3FCy1MsbEv"
      },
      "source": [
        " We plot the clustering dataset below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH_nTMKksbEv"
      },
      "outputs": [],
      "source": [
        "# Create a new matplotlib figure with a fixed size of 5x5 inches.\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "# Set the y-axis range from 0 to 2π to match the domain of the feature space.\n",
        "plt.ylim(0, 2 * np.pi)\n",
        "\n",
        "# Set the x-axis range from 0 to 2π to match the domain of the feature space.\n",
        "plt.xlim(0, 2 * np.pi)\n",
        "\n",
        "# Display the full feature space as a background heatmap.\n",
        "# - adhoc_total is the 2D similarity/score map of the ad hoc data distribution.\n",
        "# - Transpose it so that rows and columns are oriented correctly.\n",
        "# - 'RdBu' colormap shows class separation (e.g., red vs blue).\n",
        "plt.imshow(\n",
        "    np.asmatrix(adhoc_total).T,         # Transpose and convert to matrix for visualization\n",
        "    interpolation=\"nearest\",            # Use nearest-neighbor interpolation for sharp pixel blocks\n",
        "    origin=\"lower\",                     # Place (0,0) at the bottom-left corner\n",
        "    cmap=\"RdBu\",                        # Use red-blue colormap for visual contrast between classes\n",
        "    extent=[0, 2 * np.pi, 0, 2 * np.pi] # Set display range to match feature space\n",
        ")\n",
        "\n",
        "# Plot training samples with label 0 (class A).\n",
        "# Marker: square ('s'), white fill, blue edge, labeled as 'B' (can be customized).\n",
        "plot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"B\")\n",
        "\n",
        "# Plot training samples with label 1 (class B).\n",
        "# Marker: circle ('o'), white fill, red edge, also labeled as 'B' (likely meant to be 'A' and 'B').\n",
        "plot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B\")\n",
        "\n",
        "# Add a legend to the plot, positioned outside the top-right corner of the axes.\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
        "\n",
        "# Set the title of the plot to indicate this dataset is used for clustering.\n",
        "plt.title(\"Ad hoc dataset for clustering\")\n",
        "\n",
        "# Display the final plot with background heatmap and training sample markers.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWL6UUpwsbEv"
      },
      "source": [
        "### 2.2. Defining the Quantum Kernel\n",
        "We use an identical setup as in the classification example. We create another instance of the `FidelityQuantumKernel` class with a `ZZFeatureMap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VIRbqXT4sbEv"
      },
      "outputs": [],
      "source": [
        "# Create a quantum feature map using the ZZFeatureMap.\n",
        "# This map encodes classical input features into quantum states using:\n",
        "# - feature_dimension: number of input features (qubits),\n",
        "# - reps: number of repetitions of the encoding and entanglement layers,\n",
        "# - entanglement: specifies how qubits are entangled; \"linear\" connects them in a chain.\n",
        "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
        "\n",
        "# Construct a FidelityQuantumKernel using the defined feature map and fidelity function.\n",
        "# This kernel computes similarity between data points based on the fidelity of their\n",
        "# corresponding quantum states after applying the feature map.\n",
        "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwB-KENsbEv"
      },
      "source": [
        "### 2.3. Clustering with the Spectral Clustering Model\n",
        "\n",
        "The `scikit-learn` spectral clustering algorithm allows us to define a custom kernel in two ways (just like `SVC`):\n",
        "\n",
        "1. by providing the kernel as a **callable function**\n",
        "2. by precomputing the **kernel matrix**.\n",
        "\n",
        "With the current `FidelityQuantumKernel` class in `qiskit-machine-learning`, we can only use the latter option, so we precompute the kernel matrix by calling `evaluate` and visualize it as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXK5V3pLsbEv"
      },
      "outputs": [],
      "source": [
        "# Evaluate the quantum kernel on the training dataset.\n",
        "# This computes the kernel (similarity) matrix where each entry [i, j]\n",
        "# represents the fidelity between the quantum states of training samples i and j.\n",
        "adhoc_matrix = adhoc_kernel.evaluate(x_vec=train_features)\n",
        "\n",
        "# Create a new figure for plotting with a fixed size of 5x5 inches.\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "# Display the kernel matrix as an image.\n",
        "# - Convert the kernel matrix to a matrix type for compatibility with imshow.\n",
        "# - Use 'nearest' interpolation for a blocky, pixel-like appearance.\n",
        "# - Set the origin to the top-left corner.\n",
        "# - Use the 'Greens' colormap to visualize similarity values (darker = higher similarity).\n",
        "plt.imshow(np.asmatrix(adhoc_matrix), interpolation=\"nearest\", origin=\"upper\", cmap=\"Greens\")\n",
        "\n",
        "# Set the title of the plot to indicate this is the kernel matrix used for clustering.\n",
        "plt.title(\"Ad hoc clustering kernel matrix\")\n",
        "\n",
        "# Render and display the plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhUA1iSbsbEv"
      },
      "source": [
        "Next, we define a spectral clustering model and fit it using the precomputed kernel. Further, we score the labels using normalized mutual information, since we know the class labels a priori (before hand)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFHDNnBHsbEv"
      },
      "outputs": [],
      "source": [
        "# Import the SpectralClustering algorithm from scikit-learn.\n",
        "# This unsupervised learning algorithm clusters data based on the eigenstructure\n",
        "# of a similarity (kernel) matrix.\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "# Import a clustering evaluation metric.\n",
        "# Normalized Mutual Information (NMI) measures the similarity between the predicted clusters\n",
        "# and the true labels, with a score between 0 (no match) and 1 (perfect match).\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "# Initialize the SpectralClustering model with:\n",
        "# - 2 clusters (since the ad hoc dataset has 2 classes),\n",
        "# - 'precomputed' affinity, meaning the kernel matrix is passed directly instead of raw data.\n",
        "adhoc_spectral = SpectralClustering(2, affinity=\"precomputed\")\n",
        "\n",
        "# Fit the model using the quantum kernel matrix and predict cluster assignments.\n",
        "# The result is an array of cluster labels assigned to each sample.\n",
        "cluster_labels = adhoc_spectral.fit_predict(adhoc_matrix)\n",
        "\n",
        "# Evaluate the clustering result by comparing predicted cluster labels\n",
        "# to the ground truth labels using Normalized Mutual Information (NMI).\n",
        "# This metric is commonly used in clustering tasks where labels are known for evaluation only.\n",
        "cluster_score = normalized_mutual_info_score(cluster_labels, train_labels)\n",
        "\n",
        "# Print the clustering performance score (NMI), ranging from 0 (poor clustering)\n",
        "# to 1 (perfect alignment with true class labels).\n",
        "print(f\"Clustering score: {cluster_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2LuwShesbEv"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Kernel Principal Component Analysis\n",
        "\n",
        "This section focuses on a Principal Component Analysis task using a kernel PCA algorithm. We calculate a kernel matrix using a `ZZFeatureMap` and show that this approach translates the original features into a new space, where axes are chosen along principal components. In this space the classification task can be performed with a simpler model rather than an SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk7TNA33sbEv"
      },
      "source": [
        "### 3.1. Defining the dataset\n",
        "\n",
        "We again use the _ad hoc dataset_ with a gap of `0.6` between the two classes. This dataset resembles the dataset we had in the clustering section, the difference is that in this case `test_size` is not zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wi5OweK_sbEv"
      },
      "outputs": [],
      "source": [
        "# Set the number of input features (and qubits) for the dataset to 2.\n",
        "# This means each data point will be represented in a 2-dimensional feature space.\n",
        "adhoc_dimension = 2\n",
        "\n",
        "# Generate a synthetic \"ad hoc\" dataset using Qiskit's ad_hoc_data function.\n",
        "# This dataset is used to benchmark quantum machine learning models.\n",
        "\n",
        "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
        "    training_size=25,       # Number of training samples per class\n",
        "    test_size=10,           # Number of test samples per class\n",
        "    n=adhoc_dimension,      # Number of features (i.e., qubits)\n",
        "    gap=0.6,                # Controls the separation between classes; higher gap = easier separation\n",
        "    plot_data=False,        # Do not display the data plot immediately\n",
        "    one_hot=False,          # Return class labels as integers (0 or 1), not one-hot vectors\n",
        "    include_sample_total=True  # Also return the full background sample space (for visualization)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYHsyuT1sbEw"
      },
      "source": [
        "We plot the training and test datasets below. Our ultimate goal in this section is to construct new coordinates where the two classes can be linearly separated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH_ORXjlsbEw"
      },
      "outputs": [],
      "source": [
        "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOONnDB6sbEw"
      },
      "source": [
        "### 3.2. Defining the Quantum Kernel\n",
        "\n",
        "We proceed with the same kernel setup as it was in the classification task, namely a `ZZFeatureMap` circuit as a feature map and an instance of `FidelityQuantumKernel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mhdk_kPgsbEw"
      },
      "outputs": [],
      "source": [
        "# Create a quantum feature map using the ZZFeatureMap circuit.\n",
        "# This map encodes classical data into quantum states through parameterized rotations\n",
        "# and ZZ entangling interactions.\n",
        "# Parameters:\n",
        "# - feature_dimension=2: The number of input features (and qubits) is 2.\n",
        "# - reps=2: The encoding block (rotation + entanglement) is repeated twice to increase expressiveness.\n",
        "# - entanglement=\"linear\": Qubits are entangled in a linear chain (e.g., qubit 0 <-> qubit 1).\n",
        "feature_map = ZZFeatureMap(feature_dimension=2, reps=2, entanglement=\"linear\")\n",
        "\n",
        "# Create a FidelityQuantumKernel instance using the defined feature map and fidelity estimator.\n",
        "# This kernel will compute the similarity (fidelity) between quantum states resulting from the\n",
        "# feature map, and can be used in quantum-enhanced unsupervised algorithms such as quantum PCA.\n",
        "qpca_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVgqKxGsbEw"
      },
      "source": [
        "Then, we evaluate kernel matrices for the training and test features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TTJX0JjJsbEw"
      },
      "outputs": [],
      "source": [
        "# Compute the quantum kernel matrix for the training dataset using the qpca_kernel.\n",
        "# This results in a square matrix where each element [i, j] represents the fidelity-based\n",
        "# similarity between the i-th and j-th training samples.\n",
        "matrix_train = qpca_kernel.evaluate(x_vec=train_features)\n",
        "\n",
        "# Compute the quantum kernel matrix between the test and training datasets.\n",
        "# Each element [i, j] in this matrix represents the similarity between the i-th test sample\n",
        "# and the j-th training sample. This is used for evaluating model performance on unseen data.\n",
        "matrix_test = qpca_kernel.evaluate(x_vec=test_features, y_vec=train_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGua_YwWsbEw"
      },
      "source": [
        "### 3.3. Comparison of Kernel PCA on gaussian and quantum kernel\n",
        "\n",
        "In this section we use the `KernelPCA` implementation from `scikit-learn`, with the `kernel` parameter set to \"rbf\" for a gaussian kernel and \"precomputed\" for a quantum kernel. The former is very popular in classical machine learning models, whereas the latter allows using a quantum kernel defined as `qpca_kernel`.\n",
        "\n",
        "One can observe that the gaussian kernel based Kernel PCA model fails to make the dataset linearly separable, while the quantum kernel succeeds.\n",
        "\n",
        "While usually PCA is used to reduce the number of features in a dataset, or in other words to reduce dimensionality of a dataset, we don't do that here. Rather we keep the number of dimensions and employ the kernel PCA, mostly for visualization purposes, to show that classification on the transformed dataset becomes easily tractable by linear methods, like logistic regression. We use this method to separate two classes in the principal component space with a `LogisticRegression` model from `scikit-learn`. As usual we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ixnd589TsbEw"
      },
      "outputs": [],
      "source": [
        "# Import Kernel Principal Component Analysis (KernelPCA) from scikit-learn.\n",
        "# KernelPCA is a non-linear dimensionality reduction technique that projects data\n",
        "# into a lower-dimensional space using a specified kernel function.\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Create a KernelPCA instance using the classical RBF (radial basis function) kernel.\n",
        "# - n_components=2: Reduce the data to 2 principal components.\n",
        "# - kernel=\"rbf\": Use the RBF kernel to capture non-linear relationships in classical data.\n",
        "kernel_pca_rbf = KernelPCA(n_components=2, kernel=\"rbf\")\n",
        "\n",
        "# Fit the RBF-based KernelPCA model on the training features and learn the principal components.\n",
        "kernel_pca_rbf.fit(train_features)\n",
        "\n",
        "# Transform the training data into the 2D space defined by the learned RBF kernel components.\n",
        "train_features_rbf = kernel_pca_rbf.transform(train_features)\n",
        "\n",
        "# Transform the test data using the same projection learned from the training data.\n",
        "test_features_rbf = kernel_pca_rbf.transform(test_features)\n",
        "\n",
        "# Create a KernelPCA instance using a precomputed kernel matrix, which allows\n",
        "# the use of quantum kernels or any custom similarity matrix.\n",
        "kernel_pca_q = KernelPCA(n_components=2, kernel=\"precomputed\")\n",
        "\n",
        "# Apply quantum KernelPCA using the precomputed quantum kernel matrix for the training set.\n",
        "# This reduces the quantum kernel space to 2 dimensions.\n",
        "train_features_q = kernel_pca_q.fit_transform(matrix_train)\n",
        "\n",
        "# Project the test samples into the same quantum-reduced 2D space using the test-vs-train kernel matrix.\n",
        "test_features_q = kernel_pca_q.transform(matrix_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XruMV10GsbEw"
      },
      "source": [
        "Here we train and score a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K8gk2FDsbEw"
      },
      "outputs": [],
      "source": [
        "# Import the LogisticRegression model from scikit-learn.\n",
        "# Logistic regression is a simple and widely used linear classifier for binary and multiclass classification tasks.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create an instance of the LogisticRegression classifier using default parameters.\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "# Train the logistic regression model using the quantum kernel PCA-transformed training features\n",
        "# and the corresponding class labels.\n",
        "logistic_regression.fit(train_features_q, train_labels)\n",
        "\n",
        "# Evaluate the trained model on the quantum kernel PCA-transformed test features.\n",
        "# The .score() method returns the classification accuracy: correct predictions / total predictions.\n",
        "logistic_score = logistic_regression.score(test_features_q, test_labels)\n",
        "\n",
        "# Print the accuracy score of the logistic regression classifier.\n",
        "print(f\"Logistic regression score: {logistic_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUeqchsDsbEx"
      },
      "source": [
        "Let's plot the results. First, we plot the transformed dataset we get with the quantum kernel. On the same plot we also add model results. Then, we plot the transformed dataset we get with the gaussian kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaWBNYIqsbEx"
      },
      "outputs": [],
      "source": [
        "# Create a figure with two subplots placed side by side (1 row, 2 columns).\n",
        "# The figure is 10 inches wide and 5 inches tall.\n",
        "# q_ax: subplot for the quantum kernel projection\n",
        "# rbf_ax: subplot for the classical RBF kernel projection\n",
        "fig, (q_ax, rbf_ax) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# --- QUANTUM KERNEL PCA PROJECTION PLOT ---\n",
        "\n",
        "# Plot training samples for class 0 (label A) on the quantum PCA projection\n",
        "# using square white markers with blue edges\n",
        "plot_features(q_ax, train_features_q, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
        "\n",
        "# Plot training samples for class 1 (label B) on the quantum PCA projection\n",
        "# using circular white markers with red edges\n",
        "plot_features(q_ax, train_features_q, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
        "\n",
        "# Plot test samples for class 0 using blue-filled squares with white edges\n",
        "plot_features(q_ax, test_features_q, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
        "\n",
        "# Plot test samples for class 1 using red-filled circles with white edges\n",
        "plot_features(q_ax, test_features_q, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
        "\n",
        "# Label the axes and set the title of the quantum kernel PCA projection subplot\n",
        "q_ax.set_ylabel(\"Principal component #1\")\n",
        "q_ax.set_xlabel(\"Principal component #0\")\n",
        "q_ax.set_title(\"Projection of training and test data\\n using KPCA with Quantum Kernel\")\n",
        "\n",
        "# --- LINEAR DECISION BOUNDARY OVER QUANTUM PCA PROJECTION ---\n",
        "\n",
        "# Define the step size for the mesh grid used to visualize decision boundaries\n",
        "h = 0.01\n",
        "\n",
        "# Create a 2D mesh grid that spans the feature space for visualization\n",
        "x_min, x_max = train_features_q[:, 0].min() - 1, train_features_q[:, 0].max() + 1\n",
        "y_min, y_max = train_features_q[:, 1].min() - 1, train_features_q[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict the labels across the entire mesh grid using the trained logistic regression model\n",
        "# This creates a decision surface over the projected 2D space\n",
        "predictions = logistic_regression.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape the prediction results to match the shape of the mesh grid\n",
        "predictions = predictions.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary as a shaded contour area\n",
        "q_ax.contourf(xx, yy, predictions, cmap=plt.cm.RdBu, alpha=0.2)\n",
        "\n",
        "# --- CLASSICAL RBF KERNEL PCA PROJECTION PLOT ---\n",
        "\n",
        "# Plot training samples for class 0 on the RBF kernel PCA projection\n",
        "plot_features(rbf_ax, train_features_rbf, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
        "\n",
        "# Plot training samples for class 1 on the RBF kernel PCA projection\n",
        "plot_features(rbf_ax, train_features_rbf, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
        "\n",
        "# Plot test samples for class 0 on the RBF PCA projection\n",
        "plot_features(rbf_ax, test_features_rbf, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
        "\n",
        "# Plot test samples for class 1 on the RBF PCA projection\n",
        "plot_features(rbf_ax, test_features_rbf, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
        "\n",
        "# Label the axes and set the title of the RBF kernel PCA projection subplot\n",
        "rbf_ax.set_ylabel(\"Principal component #1\")\n",
        "rbf_ax.set_xlabel(\"Principal component #0\")\n",
        "rbf_ax.set_title(\"Projection of training data\\n using KernelPCA\")\n",
        "\n",
        "# Display both subplots with their corresponding projections and overlays\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEMS9tdNsbEx"
      },
      "source": [
        "As we can see, the data points on the right figure are not separable, but they are on the left figure, hence in case of quantum kernel we can apply linear models on the transformed dataset and this is why SVM classifier works perfectly well on the _ad hoc_ dataset as we saw in the classification example.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwZ_ZzXUsbEx"
      },
      "source": [
        "For further reference, `scikit-learn` has other algorithms that can use a precomputed kernel matrix, such as:\n",
        "\n",
        "- [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
        "- [Support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
        "- [Ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)\n",
        "- [Gaussian process regression](https://scikit-learn.org/stable/modules/gaussian_process.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvJrSjVOuwkg"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Quantum Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brilliant-cross"
      },
      "source": [
        "### 4.1. Quantum vs. Classical Neural Networks\n",
        "\n",
        "Classical neural networks are algorithmic models inspired by the human brain that can be trained to recognize patterns in data and learn to solve complex problems. They are based on a series of interconnected nodes, or *neurons*, organized in a layered structure, with parameters that can be learned by applying machine or deep learning training strategies.\n",
        "\n",
        "The motivation behind quantum machine learning (QML) is to integrate notions from quantum computing and classical machine learning to open the way for new and improved learning schemes. QNNs apply this generic principle by combining classical neural networks and parametrized quantum circuits. Because they lie at an intersection between two fields, QNNs can be viewed from two perspectives:\n",
        "\n",
        "- From a **machine learning perspective**, QNNs are, once again, algorithmic models that can be trained to find hidden patterns in data in a similar manner to their classical counterparts. These models can **load** classical data (**inputs**) into a quantum state, and later **process** it with quantum gates parametrized by **trainable weights**. Figure 1 shows a generic QNN example including the data loading and processing steps. The output from measuring this state can then be plugged into a loss function to train the weights through backpropagation.\n",
        "\n",
        "- From a **quantum computing perspective**, QNNs are quantum algorithms based on parametrized quantum circuits that can be trained in a variational manner using classical optimizers. These circuits contain a **feature map** (with input parameters) and an **ansatz** (with trainable weights), as seen in Figure 1.\n",
        "\n",
        "![screenshot_1747225917675.png](<https://media-hosting.imagekit.io/ffaa13fb70e9409d/screenshot_1747225917675.png?Expires=1841833917&Key-Pair-Id=K2ZIVPTIP2VGHC&Signature=X52olyjwCq7q-jwjj3fNYuc~CGHdslnvH3Zu4oGmoPEIWo8dGrIcfiC9d2adNeEGU6SO3fQz6Vw~aQEoGr0QusTbdKBCqPCmNVGAkGxJYMQIB0INkHGW7w4u1psos~oLBTrL2WmQIbhiSKgMiDnTRirEVDqxJAgJ8Pa-55ksOZBH7Pk0iLSwMbJDxh7E4QKTTY4jJbZ5lXont0meDJwRm11o04GmtlqISz0lYDoSfW-NzPpCEwEtbOhS~fhyEzVWD49izlqgjQ~SJRrxo7ct0rXaJbaz2af7Of2y2FhD8NXzS8e~IpZk49Obzp1yv4hLDWDMz6EdjVytG6GAmF0LQA__>)\n",
        "\n",
        "*Figure 1. Generic quantum neural network (QNN) structure.*\n",
        "\n",
        "As you can see, these two perspectives are complementary, and do not necessarily rely on strict definitions of concepts such as \"quantum neuron\" or what constitutes a QNN's \"layer\".\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47d2070"
      },
      "source": [
        "### 4.2. Implementation in `qiskit-machine-learning`\n",
        "\n",
        "The QNNs in `qiskit-machine-learning` are meant as application-agnostic computational units that can be used for different use cases, and their setup will depend on the application they are needed for. The module contains an interface for the QNNs and two specific implementations:\n",
        "\n",
        "1. [NeuralNetwork](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.neural_networks.NeuralNetwork.html): The interface for neural networks. This is an abstract class all QNNs inherit from.\n",
        "2. [EstimatorQNN](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.neural_networks.EstimatorQNN.html): A network based on the evaluation of quantum mechanical observables.\n",
        "3. [SamplerQNN](https://qiskit-community.github.io/qiskit-machine-learning/locale/fr_FR/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html): A network based on the samples resulting from measuring a quantum circuit.\n",
        "\n",
        "\n",
        "These implementations are based on the [qiskit primitives](https://docs.quantum.ibm.com/api/qiskit/primitives). The primitives are the entry point to run QNNs on either a simulator or real quantum hardware. Each implementation, `EstimatorQNN` and `SamplerQNN`, takes in an optional instance of its corresponding primitive, which can be any subclass of `BaseEstimator` and `BaseSampler`, respectively.\n",
        "\n",
        "The `qiskit.primitives` module provides a reference implementation for the `Sampler` and `Estimator` classes to run statevector simulations. By default, if no instance is passed to a QNN class, an instance of the corresponding reference primitive (`Sampler` or `Estimator`) is created automatically by the network.\n",
        "For more information about primitives please refer to the [primitives documentation](https://docs.quantum.ibm.com/api/qiskit/primitives).\n",
        "\n",
        "The `NeuralNetwork` class is the interface for all QNNs available in `qiskit-machine-learning`.\n",
        "It exposes a forward and a backward pass that take data samples and trainable weights as input.\n",
        "\n",
        "It's important to note that `NeuralNetwork`s are \"stateless\". They do not contain any training capabilities (these are pushed to the actual algorithms or applications: [classifiers](https://qiskit-community.github.io/qiskit-machine-learning/apidocs/qiskit_machine_learning.algorithms.html#classifiers), [regressors](https://qiskit-community.github.io/qiskit-machine-learning/apidocs/qiskit_machine_learning.algorithms.html#regressors), etc), nor do they store the values for trainable weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba316207"
      },
      "source": [
        "***\n",
        "\n",
        "Let's now look into specific examples for the two `NeuralNetwork` implementations. But first, let's set the algorithmic seed to ensure that the results don't change between runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UnMH5oN4yMe0"
      },
      "outputs": [],
      "source": [
        "# Import the algorithm_globals utility from Qiskit Machine Learning.\n",
        "# This module provides global configuration settings for Qiskit ML algorithms,\n",
        "# including the ability to control random number generation for reproducibility.\n",
        "from qiskit_machine_learning.utils import algorithm_globals\n",
        "\n",
        "# Set the global random seed to 42 to ensure reproducibility.\n",
        "# This affects any component that relies on randomness (e.g., data shuffling, parameter initialization),\n",
        "# making the results consistent across multiple runs.\n",
        "algorithm_globals.random_seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fjXjkNbCyP7J"
      },
      "outputs": [],
      "source": [
        "# DONT FORGET TO INSTALL pylatexenc!!\n",
        "!pip install pylatexenc\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h88zJpOLwewb"
      },
      "source": [
        "### 4.3. How to Instantiate QNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "billion-uniform"
      },
      "source": [
        "####  **4.3.1. `EstimatorQNN`**\n",
        "\n",
        "The `EstimatorQNN` takes in a parametrized quantum circuit as input, as well as an optional quantum mechanical observable, and outputs expectation value computations for the forward pass. The `EstimatorQNN` also accepts lists of observables to construct more complex QNNs.\n",
        "\n",
        "Let's see an `EstimatorQNN` in action with a simple example. We start by constructing the parametrized circuit. This quantum circuit has two parameters, one represents a QNN input and the other represents a trainable weight:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25o-nKBLyWQJ"
      },
      "outputs": [],
      "source": [
        "# Import the Parameter class from Qiskit, used to create symbolic (parameterized) gate values\n",
        "# that can be assigned numerical values later.\n",
        "from qiskit.circuit import Parameter\n",
        "\n",
        "# Import the QuantumCircuit class to construct quantum circuits.\n",
        "from qiskit import QuantumCircuit\n",
        "\n",
        "# Define two symbolic parameters: one for the input feature and one for the trainable weight.\n",
        "params1 = [Parameter(\"input1\"), Parameter(\"weight1\")]\n",
        "\n",
        "# Create a single-qubit quantum circuit.\n",
        "qc1 = QuantumCircuit(1)\n",
        "\n",
        "# Apply a Hadamard gate to qubit 0 to put it into superposition.\n",
        "qc1.h(0)\n",
        "\n",
        "# Apply a rotation around the Y-axis using the symbolic input parameter.\n",
        "qc1.ry(params1[0], 0)\n",
        "\n",
        "# Apply a rotation around the X-axis using the symbolic weight parameter.\n",
        "qc1.rx(params1[1], 0)\n",
        "\n",
        "# Draw the quantum circuit using the matplotlib backend with the 'clifford' style.\n",
        "# This visually displays the circuit with gate names and their parameter labels.\n",
        "qc1.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crucial-aquatic"
      },
      "source": [
        "We can now create an observable to define the expectation value computation. If not set, then the `EstimatorQNN` will automatically create the default observable $Z^{\\otimes n}$. Here, $n$ is the number of qubits of the quantum circuit.\n",
        "\n",
        "In this example, we will change things up and use the $Y^{\\otimes n}$ observable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BsR-wphlyZ8T"
      },
      "outputs": [],
      "source": [
        "# Import the SparsePauliOp class from Qiskit's quantum_info module.\n",
        "# SparsePauliOp represents a sum of Pauli operators with complex coefficients,\n",
        "# and is often used to define observables (e.g., Hamiltonians or measurement operators).\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "\n",
        "# Create an observable represented by a single Pauli Y operator applied to each qubit in the circuit.\n",
        "# - \"Y\" * qc1.num_qubits generates a string like \"Y\" or \"YY\", depending on the number of qubits.\n",
        "# - The coefficient '1' assigns a scalar multiplier to the operator.\n",
        "# - This observable will be used, for example, to measure the expectation value of the state\n",
        "#   generated by the quantum circuit qc1 with respect to the Pauli-Y basis.\n",
        "observable1 = SparsePauliOp.from_list([(\"Y\" * qc1.num_qubits, 1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1fb7ea"
      },
      "source": [
        "Together with the quantum circuit defined above, and the observable we have created, the `EstimatorQNN` constructor takes in the following keyword arguments:\n",
        "\n",
        "- `estimator`: optional primitive instance\n",
        "- `pass_manager`: optional pass_manager instance for primitives that require transpilation\n",
        "- `input_params`: list of quantum circuit parameters that should be treated as \"network inputs\"\n",
        "- `weight_params`: list of quantum circuit parameters that should be treated as \"network weights\"\n",
        "\n",
        "In this example, we previously decided that the first parameter of `params1` should be the input, while the second should be the weight. As we are performing a local statevector simulation, we will set the `estimator` parameter from `qiskit.primitives.StatevectorEstimator`. If we needed to access cloud resources or `Aer` simulators, we would have to define the respective `Estimator` instances and pass them to the `EstimatorQNN`. If transpilation is required an estimator such as `qiskit_ibm_runtime.EstimatorV2`, it is required to pass a `pass_manager` to the `EstimatorQNN`. In such case, please do not forget that  `observables` might require new layout according to the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msJ52OToyc6Q"
      },
      "outputs": [],
      "source": [
        "# Import the EstimatorQNN class from Qiskit Machine Learning.\n",
        "# EstimatorQNN represents a parameterized quantum neural network based on an expectation value computation.\n",
        "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
        "\n",
        "# Import the StatevectorEstimator primitive, which computes exact expectation values\n",
        "# of observables from quantum circuits using ideal (noise-free) statevectors.\n",
        "from qiskit.primitives import StatevectorEstimator as Estimator\n",
        "\n",
        "# Instantiate the estimator backend.\n",
        "# This primitive will be used by the QNN to evaluate expectation values.\n",
        "estimator = Estimator()\n",
        "\n",
        "# Create an Estimator Quantum Neural Network (EstimatorQNN).\n",
        "# This QNN computes the expectation value of a given observable after executing the quantum circuit.\n",
        "# Parameters:\n",
        "# - circuit: the quantum circuit (qc1) that contains parameterized input and weights.\n",
        "# - observables: the operator (observable1) whose expectation value is to be computed.\n",
        "# - input_params: parameters in the circuit that represent the input data (e.g., \"input1\").\n",
        "# - weight_params: parameters in the circuit that represent trainable weights (e.g., \"weight1\").\n",
        "# - estimator: the backend (simulator or hardware) used to evaluate the circuit and compute expectation values.\n",
        "estimator_qnn = EstimatorQNN(\n",
        "    circuit=qc1,\n",
        "    observables=observable1,\n",
        "    input_params=[params1[0]],\n",
        "    weight_params=[params1[1]],\n",
        "    estimator=estimator,\n",
        ")\n",
        "\n",
        "# Display the EstimatorQNN object.\n",
        "# This object now acts as a quantum neural network that maps input data and weights\n",
        "# to an output value via quantum expectation calculation.\n",
        "estimator_qnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ecacdb"
      },
      "source": [
        "We'll see how to use the QNN in the following sections, but before that, let's check out the `SamplerQNN` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ac3811"
      },
      "source": [
        "#### 4.3.2. `SamplerQNN`\n",
        "\n",
        "The `SamplerQNN` is instantiated in a similar way to the `EstimatorQNN`, but because it directly consumes samples from measuring the quantum circuit, it does not require a custom observable.\n",
        "\n",
        "These output samples are interpreted by default as the probabilities of measuring the integer index corresponding to a bitstring. However, the `SamplerQNN` also allows us to specify an `interpret` function to post-process the samples. This function should be defined so that it takes a measured integer (from a bitstring) and maps it to a new value, i.e. non-negative integer.\n",
        "\n",
        "**(!)** It's important to note that if a custom `interpret` function is defined, the `output_shape` cannot be inferred by the network, and **needs to be provided explicitly**.\n",
        "\n",
        "**(!)** It's also important to keep in mind that if no `interpret` function is used, the dimension of the probability vector will scale exponentially with the number of qubits. With a custom `interpret` function, this scaling can change. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, the result will be a probability vector of length 2 independently of the number of qubits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82cc2353"
      },
      "source": [
        "Let's create a different quantum circuit for the `SamplerQNN`. In this case, we will have two input parameters and four trainable weights that parametrize a  two-local circuit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDZQvKAmyiwO"
      },
      "outputs": [],
      "source": [
        "# Import the ParameterVector class to define a list of symbolic parameters for the quantum circuit.\n",
        "# ParameterVector allows convenient creation and indexing of multiple named parameters.\n",
        "from qiskit.circuit import ParameterVector\n",
        "\n",
        "# Create a vector of 2 input parameters named \"input\", typically used to encode classical input data.\n",
        "inputs2 = ParameterVector(\"input\", 2)\n",
        "\n",
        "# Create a vector of 4 weight parameters named \"weight\", typically used as trainable model parameters.\n",
        "weights2 = ParameterVector(\"weight\", 4)\n",
        "\n",
        "# Print the names of the input parameters (e.g., ['input[0]', 'input[1]']) for verification.\n",
        "print(f\"input parameters: {[str(item) for item in inputs2.params]}\")\n",
        "\n",
        "# Print the names of the weight parameters (e.g., ['weight[0]', ..., 'weight[3]']) for verification.\n",
        "print(f\"weight parameters: {[str(item) for item in weights2.params]}\")\n",
        "\n",
        "# Create a new quantum circuit with 2 qubits.\n",
        "qc2 = QuantumCircuit(2)\n",
        "\n",
        "# Apply a rotation around the Y-axis on qubit 0 using the first input parameter.\n",
        "qc2.ry(inputs2[0], 0)\n",
        "\n",
        "# Apply a rotation around the Y-axis on qubit 1 using the second input parameter.\n",
        "qc2.ry(inputs2[1], 1)\n",
        "\n",
        "# Apply a CNOT gate (entanglement) with qubit 0 as control and qubit 1 as target.\n",
        "qc2.cx(0, 1)\n",
        "\n",
        "# Apply parameterized Ry rotations on both qubits using the first two weight parameters.\n",
        "qc2.ry(weights2[0], 0)\n",
        "qc2.ry(weights2[1], 1)\n",
        "\n",
        "# Apply a second CNOT gate to entangle the qubits again.\n",
        "qc2.cx(0, 1)\n",
        "\n",
        "# Apply additional Ry rotations on both qubits using the remaining two weight parameters.\n",
        "qc2.ry(weights2[2], 0)\n",
        "qc2.ry(weights2[3], 1)\n",
        "\n",
        "# Draw and visualize the quantum circuit using matplotlib with the 'clifford' style.\n",
        "qc2.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e86defb"
      },
      "source": [
        "Similarly to the `EstimatorQNN`, we must specify inputs and weights when instantiating the `SamplerQNN`. In this case, the keyword arguments will be:\n",
        "\n",
        "- `sampler`: optional primitive instance\n",
        "- `pass_manager`: optional pass_manager instance for primitives that require transpilation.\n",
        "- `input_params`: list of quantum circuit parameters that should be treated as \"network inputs\"\n",
        "- `weight_params`: list of quantum circuit parameters that should be treated as \"network weights\"\n",
        "\n",
        "\n",
        "Please note that, once again, we are setting the `StatevectorSampler` instance from `qiskit.primitives` to the QNN and relying on `statevector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnrucoEqylZ7"
      },
      "outputs": [],
      "source": [
        "# Import the SamplerQNN class from Qiskit Machine Learning.\n",
        "# SamplerQNN is a quantum neural network that uses the output probabilities from a quantum circuit (via sampling)\n",
        "# to generate predictions. It is often used for classification tasks.\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
        "\n",
        "# Import the StatevectorSampler from Qiskit primitives.\n",
        "# This simulator returns exact probability distributions (no noise) by simulating the full quantum state.\n",
        "from qiskit.primitives import StatevectorSampler as Sampler\n",
        "\n",
        "# Instantiate the ideal (noise-free) sampler backend using statevector simulation.\n",
        "# This will be used to evaluate the quantum circuit's output probabilities.\n",
        "sampler = Sampler()\n",
        "\n",
        "# Create a Sampler-based Quantum Neural Network using the previously defined quantum circuit `qc2`.\n",
        "# Parameters:\n",
        "# - circuit: the parameterized quantum circuit (qc2) with symbolic input and weight parameters.\n",
        "# - input_params: the list of parameters in the circuit that correspond to classical inputs (features).\n",
        "# - weight_params: the list of parameters that represent trainable weights.\n",
        "# - sampler: the backend that evaluates the circuit and returns output probabilities.\n",
        "sampler_qnn = SamplerQNN(\n",
        "    circuit=qc2,\n",
        "    input_params=inputs2,\n",
        "    weight_params=weights2,\n",
        "    sampler=sampler\n",
        "\n",
        ")\n",
        "\n",
        "# Display the SamplerQNN object, which can now be used as a quantum model\n",
        "# in machine learning workflows (e.g., wrapped in a TorchConnector or used with optimizers).\n",
        "sampler_qnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8c56e76"
      },
      "source": [
        "In addition to the basic arguments shown above, the `SamplerQNN` accepts three more settings: `input_gradients`, `interpret`, and `output_shape`. These will be introduced in sections 4 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ac89b99"
      },
      "source": [
        "### 4.4. How to Run a Forward Pass\n",
        "\n",
        "Running a **forward pass** in a quantum neural network (QNN) refers to the process of evaluating the network's output given a specific set of input features and weight parameters. In the context of Qiskit Machine Learning, this is typically done using the `.forward()` method provided by QNN classes such as `EstimatorQNN` or `SamplerQNN`. The forward pass computes the output of the quantum circuit—either as an expectation value (for `EstimatorQNN`) or as a probability distribution (for `SamplerQNN`)—based on how the input data and trainable weights are embedded into the quantum circuit. This step is crucial both for prediction and for training, as it provides the values needed to calculate a loss function or evaluate model performance. Before running a forward pass, the user must ensure that the input and weight vectors match the number of parameters expected by the QNN. In practice, these vectors may come from a dataset or be initialized randomly for demonstration purposes, as shown in the tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8589abbd"
      },
      "source": [
        "### 4.4.1. Set-Up\n",
        "\n",
        "In a real setting, the inputs would be defined by the dataset, and the weights would be defined by the training algorithm or as part of a pre-trained model. However, for the sake of this tutorial, we will specify random sets of input and weights of the right dimension:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2698406f"
      },
      "source": [
        "#### 4.4.2. `EstimatorQNN` Example\n",
        "\n",
        "This example demonstrates how to perform a forward pass using the `EstimatorQNN` class from Qiskit Machine Learning. It begins by generating random input and weight vectors that match the number of parameters required by the quantum neural network. These values are created using Qiskit’s globally seeded random number generator to ensure reproducibility. The number of required input features and trainable weights is retrieved from the QNN using `estimator_qnn.num_inputs` and `estimator_qnn.num_weights`. The randomly initialized vectors are then printed for verification. This setup serves as a preparation step for feeding the input and weight values into the QNN for evaluation using its `.forward()` method. The example effectively illustrates how to initialize and inspect the data needed to simulate the network's behavior before applying it to real tasks such as training or prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FTkOJ51zyq7P"
      },
      "outputs": [],
      "source": [
        "# Generate a random input vector for the EstimatorQNN using the global random number generator.\n",
        "# - estimator_qnn.num_inputs gives the number of input parameters expected by the QNN.\n",
        "# - algorithm_globals.random provides a seeded random number generator (for reproducibility).\n",
        "# - The result is a NumPy array of shape (num_inputs,) with values in the range [0, 1).\n",
        "estimator_qnn_input = algorithm_globals.random.random(estimator_qnn.num_inputs)\n",
        "\n",
        "# Generate a random weight vector for the EstimatorQNN in the same way.\n",
        "# - estimator_qnn.num_weights returns the number of trainable weight parameters.\n",
        "# - These random weights will later be used to evaluate or train the quantum neural network.\n",
        "estimator_qnn_weights = algorithm_globals.random.random(estimator_qnn.num_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78w8QiMmysPo"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Number of input features for EstimatorQNN: {estimator_qnn.num_inputs} \\nInput: {estimator_qnn_input}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of trainable weights for EstimatorQNN: {estimator_qnn.num_weights} \\nWeights: {estimator_qnn_weights}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d07341"
      },
      "source": [
        "#### 4.4.3. `SamplerQNN` Example\n",
        "\n",
        "This example extends the forward pass procedure to the `SamplerQNN` class, which models quantum neural networks based on output probabilities rather than expectation values. Similar to the `EstimatorQNN` example, random input and weight vectors are generated using a seeded random number generator to ensure reproducibility. These vectors must match the dimensional requirements defined by the number of input and weight parameters in the QNN. The `.forward()` method of `SamplerQNN` is then used to compute the output, which consists of a probability distribution over measured quantum states. This example demonstrates how `SamplerQNN` can be used to simulate classification-like behaviors by analyzing output probabilities, laying the groundwork for further applications such as interpreting measurements or computing gradients during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "D9EB8Iqoyv9S"
      },
      "outputs": [],
      "source": [
        "# Generate a random input vector for the SamplerQNN using the global random number generator.\n",
        "# - sampler_qnn.num_inputs returns the number of input parameters expected by the quantum neural network.\n",
        "# - algorithm_globals.random is a reproducible random number generator (seeded earlier using algorithm_globals.random_seed).\n",
        "# - The result is a NumPy array of random values in the range [0, 1), one for each input parameter.\n",
        "sampler_qnn_input = algorithm_globals.random.random(sampler_qnn.num_inputs)\n",
        "\n",
        "# Generate a random weight vector for the SamplerQNN using the same random number generator.\n",
        "# - sampler_qnn.num_weights returns the number of trainable weight parameters in the QNN.\n",
        "# - This array represents a randomly initialized weight configuration that can be used for evaluation or optimization.\n",
        "sampler_qnn_weights = algorithm_globals.random.random(sampler_qnn.num_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqN-YFpyxXh"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Number of input features for SamplerQNN: {sampler_qnn.num_inputs} \\nInput: {sampler_qnn_input}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of trainable weights for SamplerQNN: {sampler_qnn.num_weights} \\nWeights: {sampler_qnn_weights}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f1500df"
      },
      "source": [
        "Once we have the inputs and the weights, let's see the results for batched and non-batched passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7e7302"
      },
      "source": [
        "### 4.5. Non-batched Forward Pass\n",
        "\n",
        "**4.5. Non-batched Forward Pass** explains how to perform a forward pass using a single input sample in both `EstimatorQNN` and `SamplerQNN` quantum neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52c5fcc5"
      },
      "source": [
        "#### 4.5.1. `EstimatorQNN` Example\n",
        "\n",
        "In the **EstimatorQNN example**, the forward pass computes the expectation value of a specified observable using the given input features and weight parameters. The result is typically a single scalar or a small array, with an output shape of `(1, num_qubits * num_observables)`, where `1` represents a single sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fba01a3"
      },
      "source": [
        "For the `EstimatorQNN`, the expected output shape for the forward pass is `(1, num_qubits * num_observables)` where `1` in our case is the number of samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxA3FQx0y2tY"
      },
      "outputs": [],
      "source": [
        "# Perform a forward pass through the Estimator Quantum Neural Network (EstimatorQNN).\n",
        "# - This computes the expected value of the observable defined in the QNN,\n",
        "#   given the input data (estimator_qnn_input) and the current weights (estimator_qnn_weights).\n",
        "# - The result is typically a single scalar value (or a small array), representing the QNN's output.\n",
        "estimator_qnn_forward = estimator_qnn.forward(estimator_qnn_input, estimator_qnn_weights)\n",
        "\n",
        "# Print the result of the forward pass.\n",
        "# - This displays both the computed value (expectation value) and its shape.\n",
        "# - The shape is useful to verify the output format, especially when integrating with classical ML tools.\n",
        "print(\n",
        "    f\"Forward pass result for EstimatorQNN: {estimator_qnn_forward}. \\nShape: {estimator_qnn_forward.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ea4f85e"
      },
      "source": [
        "#### 4.5.2. `SamplerQNN` Example\n",
        "\n",
        "\n",
        "In the **SamplerQNN example**, the forward pass produces a probability distribution over possible measurement outcomes, resulting in a vector of probabilities. If no custom `interpret` function is used, the output shape is `(1, 2^n)` for an `n`-qubit circuit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94473b35"
      },
      "source": [
        "For the `SamplerQNN` (without a custom interpret function), the expected output shape for the forward pass is `(1, 2**num_qubits)`. With a custom interpret function, the output shape will be `(1, output_shape)`, where `1` in our case is the number of samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3-Yani2y5-U"
      },
      "outputs": [],
      "source": [
        "# Perform a forward pass through the Sampler Quantum Neural Network (SamplerQNN).\n",
        "# - This computes the output probability distribution over measurement outcomes (e.g., bitstrings),\n",
        "#   given a specific input vector (sampler_qnn_input) and a set of weight parameters (sampler_qnn_weights).\n",
        "# - The result is a probability vector where each entry corresponds to the probability of a particular outcome.\n",
        "sampler_qnn_forward = sampler_qnn.forward(sampler_qnn_input, sampler_qnn_weights)\n",
        "\n",
        "# Print the result of the forward pass.\n",
        "# - This will show the predicted probability distribution and its shape.\n",
        "# - The shape typically reflects the number of possible measurement outcomes (e.g., 2^n for n qubits).\n",
        "print(\n",
        "    f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}.  \\nShape: {sampler_qnn_forward.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c843c95"
      },
      "source": [
        "### 4.5.3. Batched Forward Pass\n",
        "\n",
        "**4.5.3. Batched Forward Pass** demonstrates how to evaluate a quantum neural network (QNN) on multiple input samples simultaneously by passing a batch of inputs instead of a single one. This is useful in practical machine learning settings, where models are often trained or evaluated on batches of data for computational efficiency.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c51e2fc"
      },
      "source": [
        "#### 4.5.4. `EstimatorQNN` Example\n",
        "\n",
        "In the **EstimatorQNN** example, a list of input vectors is passed to the `.forward()` method, and the model returns a corresponding list of expectation values, one per input. The output shape reflects the batch size, confirming that the network can handle multiple samples correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3612ff46"
      },
      "source": [
        "For the `EstimatorQNN`, the expected output shape for the forward pass is `(batch_size, num_qubits * num_observables)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTXsJSQLy-Fy"
      },
      "outputs": [],
      "source": [
        "# Perform a batched forward pass through the Estimator Quantum Neural Network (EstimatorQNN).\n",
        "# - Instead of a single input vector, we pass a list of two identical input vectors.\n",
        "# - This simulates evaluating the QNN on a batch of inputs using the same set of weights.\n",
        "# - The network will compute an output (expectation value) for each input in the batch.\n",
        "estimator_qnn_forward_batched = estimator_qnn.forward(\n",
        "    [estimator_qnn_input, estimator_qnn_input],  # A list containing two input vectors (identical in this case)\n",
        "    estimator_qnn_weights                        # The same set of weights applied to both inputs\n",
        ")\n",
        "\n",
        "# Print the results of the batched forward pass.\n",
        "# - This will show an array of outputs (one per input in the batch) and its shape.\n",
        "# - The shape helps confirm that the batch dimension is being correctly handled (e.g., shape = (2,) for 2 inputs).\n",
        "print(\n",
        "    f\"Forward pass result for EstimatorQNN: {estimator_qnn_forward_batched}.  \\nShape: {estimator_qnn_forward_batched.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb7b0a7"
      },
      "source": [
        "#### 4.5.5. `SamplerQNN` Example\n",
        "\n",
        "Similarly, in the **SamplerQNN** example, the `.forward()` method produces a matrix of output probability distributions, where each row corresponds to one input sample and each column represents a possible measurement outcome. The batched forward pass ensures that QNNs integrate smoothly with classical batch-processing workflows, making them suitable for hybrid quantum-classical machine learning pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48f7b7bb"
      },
      "source": [
        "For the `SamplerQNN` (without custom interpret function), the expected output shape for the forward pass is `(batch_size, 2**num_qubits)`. With a custom interpret function, the output shape will be `(batch_size, output_shape)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_b2nqQqzBx6"
      },
      "outputs": [],
      "source": [
        "# Perform a batched forward pass through the Sampler Quantum Neural Network (SamplerQNN).\n",
        "# - Instead of passing a single input vector, we provide a list containing two identical input vectors.\n",
        "# - The SamplerQNN will evaluate both inputs using the same set of weight parameters.\n",
        "# - The result is a batch of output probability distributions, one for each input vector.\n",
        "sampler_qnn_forward_batched = sampler_qnn.forward(\n",
        "    [sampler_qnn_input, sampler_qnn_input],  # A list of two input vectors (batched input)\n",
        "    sampler_qnn_weights                      # A single set of weights used for both inputs\n",
        ")\n",
        "\n",
        "# Print the results of the batched forward pass.\n",
        "# - Each entry in the output corresponds to a probability distribution over bitstrings.\n",
        "# - The shape reflects the number of inputs in the batch and the number of output probabilities.\n",
        "print(\n",
        "    f\"Forward pass result for SamplerQNN: {sampler_qnn_forward_batched}.  \\nShape: {sampler_qnn_forward_batched.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171b8ee1"
      },
      "source": [
        "### 5. How to Run a Backward Pass\n",
        "\n",
        "Let's take advantage of the inputs and weights defined above to show how the backward pass works. This pass returns a tuple `(input_gradients, weight_gradients)`. By default, the backward pass will only calculate gradients with respect to the weight parameters.\n",
        "\n",
        "If you want to enable gradients with respect to the input parameters, you should set the following flag during the QNN instantiation:\n",
        "\n",
        "```python\n",
        "qnn = ...QNN(..., input_gradients=True)\n",
        "```\n",
        "\n",
        "Please remember that input gradients are **required** for the use of `TorchConnector` for PyTorch integration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5b90338"
      },
      "source": [
        "### 5.1. Backward Pass without Input Gradients\n",
        "\n",
        "**5.1. Backward Pass without Input Gradients** illustrates how to compute gradients of a quantum neural network’s output with respect to its **trainable weight parameters**, without enabling input gradients. This backward pass is essential for training models using gradient-based optimization methods such as gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1a6c32"
      },
      "source": [
        "#### 5.1.1. `EstimatorQNN` Example\n",
        "\n",
        "In the **EstimatorQNN example**, the `.backward()` method returns the gradient of the output with respect to both inputs and weights, but since input gradients are not explicitly enabled, the result for input gradients is `None`. The output shape of the weight gradients confirms that the network is properly computing a gradient value for each weight across the batch and observable dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "387c9700"
      },
      "source": [
        "For the `EstimatorQNN`, the expected output shape for the weight gradients is `(batch_size, num_qubits * num_observables, num_weights)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXzhkYlJzGq0"
      },
      "outputs": [],
      "source": [
        "# Compute gradients of the EstimatorQNN's output with respect to both the input and weight parameters.\n",
        "# - The .backward() method returns two values:\n",
        "#   1. Gradient of the output with respect to the input parameters (∂output/∂input).\n",
        "#   2. Gradient of the output with respect to the weight parameters (∂output/∂weights).\n",
        "# - These gradients are useful for understanding how sensitive the output is to changes in inputs or weights,\n",
        "#   and are essential for training in gradient-based optimization.\n",
        "estimator_qnn_input_grad, estimator_qnn_weight_grad = estimator_qnn.backward(\n",
        "    estimator_qnn_input,        # Input values at which to compute the gradient\n",
        "    estimator_qnn_weights       # Weight values at which to compute the gradient\n",
        ")\n",
        "\n",
        "# Print the input gradients (i.e., how much the output changes with respect to each input).\n",
        "# Note: There is a small mistake in the shape printing here; it prints the full array, not its shape.\n",
        "print(\n",
        "    f\"Input gradients for EstimatorQNN: {estimator_qnn_input_grad}.  \\nShape: {estimator_qnn_input_grad}\"\n",
        ")\n",
        "\n",
        "# Print the weight gradients and their shape.\n",
        "# These gradients are used to update weights during training using optimization algorithms like gradient descent.\n",
        "print(\n",
        "    f\"Weight gradients for EstimatorQNN: {estimator_qnn_weight_grad}.  \\nShape: {estimator_qnn_weight_grad.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d5feb04"
      },
      "source": [
        "#### 5.1.2. `SamplerQNN` Example\n",
        "\n",
        "In the **SamplerQNN example**, a similar backward pass is performed. The resulting gradient array shows how each weight affects the output probability distribution for every possible measurement outcome. This section emphasizes the utility of weight gradients for training quantum models and highlights that input gradients must be explicitly enabled if needed, as they are not computed by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bebaa404"
      },
      "source": [
        "For the `SamplerQNN` (without custom interpret function), the expected output shape for the forward pass is `(batch_size, 2**num_qubits, num_weights)`. With a custom interpret function, the output shape will be `(batch_size, output_shape, num_weights)`.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obBzT8z5zKDC"
      },
      "outputs": [],
      "source": [
        "# Compute the gradients of the SamplerQNN's output with respect to both the input and weight parameters.\n",
        "# - The .backward() method calculates how small changes in input and weight parameters affect the output.\n",
        "# - Returns:\n",
        "#   1. sampler_qnn_input_grad: gradient of the output with respect to each input parameter.\n",
        "#   2. sampler_qnn_weight_grad: gradient of the output with respect to each weight parameter.\n",
        "# - These gradients are especially useful for training the quantum neural network using gradient-based optimizers.\n",
        "sampler_qnn_input_grad, sampler_qnn_weight_grad = sampler_qnn.backward(\n",
        "    sampler_qnn_input,        # Specific input vector at which to compute the gradient\n",
        "    sampler_qnn_weights       # Corresponding weight vector\n",
        ")\n",
        "\n",
        "# Print the computed input gradients.\n",
        "# NOTE: There is a minor inconsistency here: `.shape` is expected after the gradient variable for shape,\n",
        "# but currently it prints the full gradient in both the value and shape lines.\n",
        "print(\n",
        "    f\"Input gradients for SamplerQNN: {sampler_qnn_input_grad}.  \\nShape: {sampler_qnn_input_grad}\"\n",
        ")\n",
        "\n",
        "# Print the computed weight gradients and their shape.\n",
        "# This helps verify the dimensionality of the gradients (e.g., it should match the number of weight parameters).\n",
        "print(\n",
        "    f\"Weight gradients for SamplerQNN: {sampler_qnn_weight_grad}.  \\nShape: {sampler_qnn_weight_grad.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d28a00"
      },
      "source": [
        "### 5.2. Backward Pass with Input Gradients\n",
        "\n",
        "Section **5.2. Backward Pass with Input Gradients** explains how to compute gradients of a quantum neural network (QNN) not only with respect to its trainable weights but also with respect to the input features. By default, Qiskit's QNNs only return gradients for weight parameters; however, setting the `input_gradients` attribute to `True` enables the network to compute how changes in inputs influence the output. This is especially valuable for tasks such as model interpretability, adversarial robustness analysis, and feature attribution.\n",
        "\n",
        "Let's enable the `input_gradients` to show what the expected output sizes are for this option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "D8bvkhrizMwW"
      },
      "outputs": [],
      "source": [
        "# Enable input gradient computation for the EstimatorQNN.\n",
        "# By default, Qiskit's QNNs only compute gradients with respect to weight parameters.\n",
        "# Setting input_gradients = True allows the .backward() method to also return\n",
        "# the gradient of the network output with respect to the input parameters (∂output/∂input).\n",
        "estimator_qnn.input_gradients = True\n",
        "\n",
        "# Similarly, enable input gradient computation for the SamplerQNN.\n",
        "# This allows you to track how sensitive the output probabilities are to changes in input values,\n",
        "# which is useful for tasks like adversarial robustness analysis or interpreting model behavior.\n",
        "sampler_qnn.input_gradients = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5d2cd57"
      },
      "source": [
        "#### 5.2.1. `EstimatorQNN` Example\n",
        "\n",
        "In the **EstimatorQNN example**, enabling input gradients allows the `.backward()` method to return both input and weight gradients with shapes reflecting batch size, number of observables, and number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d1ebc80"
      },
      "source": [
        "For the `EstimatorQNN`, the expected output shape for the input gradients is `(batch_size, num_qubits * num_observables, num_inputs)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXGhDZ_RzPoV"
      },
      "outputs": [],
      "source": [
        "# Compute gradients of the EstimatorQNN's output with respect to both input and weight parameters.\n",
        "# The .backward() method returns:\n",
        "# - estimator_qnn_input_grad: ∂output/∂input — how much the output changes with respect to each input.\n",
        "# - estimator_qnn_weight_grad: ∂output/∂weight — how much the output changes with respect to each trainable weight.\n",
        "# This is only possible if input gradients have been explicitly enabled (input_gradients = True).\n",
        "estimator_qnn_input_grad, estimator_qnn_weight_grad = estimator_qnn.backward(\n",
        "    estimator_qnn_input,       # Input vector to evaluate gradients at\n",
        "    estimator_qnn_weights      # Weight vector to evaluate gradients at\n",
        ")\n",
        "\n",
        "# Print the computed gradients with respect to the input parameters.\n",
        "# Also print the shape of the gradient array, which should match the number of inputs.\n",
        "print(\n",
        "    f\"Input gradients for EstimatorQNN: {estimator_qnn_input_grad}.  \\nShape: {estimator_qnn_input_grad.shape}\"\n",
        ")\n",
        "\n",
        "# Print the computed gradients with respect to the weight parameters.\n",
        "# Also print the shape of the gradient array, which should match the number of weights.\n",
        "print(\n",
        "    f\"Weight gradients for EstimatorQNN: {estimator_qnn_weight_grad}.  \\nShape: {estimator_qnn_weight_grad.shape}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e50ff8"
      },
      "source": [
        "#### 5.2.2. `SamplerQNN` Example\n",
        "\n",
        "Similarly, in the **SamplerQNN example**, gradients are computed for both input features and weights, but the shape reflects the number of possible measurement outcomes due to the probabilistic nature of the sampler. This section highlights how enabling input gradients provides deeper insight into the network's behavior and sensitivity across both data and parameter dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b76da18a"
      },
      "source": [
        "For the `SamplerQNN` (without custom interpret function), the expected output shape for the input gradients is `(batch_size, 2**num_qubits, num_inputs)`. With a custom interpret function, the output shape will be `(batch_size, output_shape, num_inputs)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5DE-H9uzSq5"
      },
      "outputs": [],
      "source": [
        "# Compute the gradients of the SamplerQNN's output with respect to both input and weight parameters.\n",
        "# This is possible because we previously set `sampler_qnn.input_gradients = True`.\n",
        "# The .backward() method returns two values:\n",
        "# - sampler_qnn_input_grad: ∂output/∂input — sensitivity of the output probability distribution\n",
        "#   with respect to each input parameter.\n",
        "# - sampler_qnn_weight_grad: ∂output/∂weight — sensitivity of the output with respect to each trainable weight.\n",
        "sampler_qnn_input_grad, sampler_qnn_weight_grad = sampler_qnn.backward(\n",
        "    sampler_qnn_input,       # Input vector at which to compute the gradient\n",
        "    sampler_qnn_weights      # Weight vector used for computing gradients\n",
        ")\n",
        "\n",
        "# Print the input gradients — i.e., how changes in each input feature affect the output distribution.\n",
        "# Also print the shape of the gradient array (should match the number of input parameters).\n",
        "print(\n",
        "    f\"Input gradients for SamplerQNN: {sampler_qnn_input_grad}.  \\nShape: {sampler_qnn_input_grad.shape}\"\n",
        ")\n",
        "\n",
        "# Print the weight gradients — i.e., how changes in each weight parameter affect the output.\n",
        "# Also print the shape of this gradient array (should match the number of weight parameters).\n",
        "print(\n",
        "    f\"Weight gradients for SamplerQNN: {sampler_qnn_weight_grad}.  \\nShape: {sampler_qnn_weight_grad.shape}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45871b6d"
      },
      "source": [
        "## 6. Advanced Functionality\n",
        "\n",
        "**Advanced Functionality** demonstrates how Qiskit's `EstimatorQNN` can be extended to handle more complex quantum neural network (QNN) architectures by incorporating **multiple observables**. This is particularly useful when a model needs to predict multiple outputs or evaluate multiple measurement operators simultaneously. In this example, an additional observable is defined using a tensor product of Pauli-Z operators, resulting in a setup where two different observables are evaluated in the same quantum circuit.\n",
        "\n",
        "The new `EstimatorQNN` instance (`estimator_qnn2`) is configured with a list of observables, and its `.forward()` method returns a vector of expectation values—one for each observable. This marks a change from the single-observable case, where the output is a scalar or single-element array. Moreover, the `.backward()` method now returns **two separate gradient arrays**: one for the input parameters and one for the weights, with dimensions that account for the number of observables. This makes the model suitable for multitask learning or architectures requiring richer output representations.\n",
        "\n",
        "The example compares the output and gradient shapes between the single-observable and multi-observable QNNs, highlighting how the QNN's dimensional behavior scales with its configuration. This example illustrates the flexibility of Qiskit's neural network interface, allowing researchers to tailor quantum models to a variety of learning tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e1fb829"
      },
      "source": [
        "### 6.1. `EstimatorQNN` with Multiple Observables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18c86fd7"
      },
      "source": [
        "The `EstimatorQNN` allows to pass lists of observables for more complex QNN architectures. For example (note the change in output shape):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssly9GnDzWkw"
      },
      "outputs": [],
      "source": [
        "# Create a second observable using a tensor product of Pauli-Z operators.\n",
        "# - \"Z\" * qc1.num_qubits creates a string of 'Z' operators equal to the number of qubits in the circuit.\n",
        "# - The coefficient 1 defines the observable as 1 * Z⊗Z (for 2 qubits), or just 1 * Z (for 1 qubit).\n",
        "# - This observable measures the expectation value in the Z basis.\n",
        "observable2 = SparsePauliOp.from_list([(\"Z\" * qc1.num_qubits, 1)])\n",
        "\n",
        "# Define a new Estimator Quantum Neural Network (EstimatorQNN) that outputs multiple expectation values.\n",
        "# - circuit: the parameterized quantum circuit to execute (qc1).\n",
        "# - observables: a list of two observables (observable1 and observable2) whose expectation values\n",
        "#   will be computed and returned as a multi-dimensional output (vector-valued QNN).\n",
        "# - input_params: parameters in the circuit that encode classical input data (e.g., \"input1\").\n",
        "# - weight_params: trainable parameters in the circuit (e.g., \"weight1\").\n",
        "# - estimator: the simulation backend used to compute the exact expectation values.\n",
        "# The resulting QNN produces a 2-dimensional output (one value per observable) for each input.\n",
        "estimator_qnn2 = EstimatorQNN(\n",
        "    circuit=qc1,\n",
        "    observables=[observable1, observable2],\n",
        "    input_params=[params1[0]],\n",
        "    weight_params=[params1[1]],\n",
        "    estimator=estimator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCS94oFTzYJi"
      },
      "outputs": [],
      "source": [
        "# Perform a forward pass through the second EstimatorQNN (estimator_qnn2),\n",
        "# which returns a vector of expectation values — one for each observable (observable1 and observable2).\n",
        "# Input and weight values are reused from the original QNN setup.\n",
        "estimator_qnn_forward2 = estimator_qnn2.forward(estimator_qnn_input, estimator_qnn_weights)\n",
        "\n",
        "# Compute the gradients of the second EstimatorQNN’s output with respect to both inputs and weights.\n",
        "# Because estimator_qnn2 has two observables, the gradients will be returned as 2D arrays:\n",
        "# - Shape: (num_observables, num_inputs) for input gradients\n",
        "# - Shape: (num_observables, num_weights) for weight gradients\n",
        "estimator_qnn_input_grad2, estimator_qnn_weight_grad2 = estimator_qnn2.backward(\n",
        "    estimator_qnn_input,\n",
        "    estimator_qnn_weights\n",
        ")\n",
        "\n",
        "# Print the shape of the output from the original EstimatorQNN (estimator_qnn),\n",
        "# which uses a single observable and therefore produces a scalar output (shape: (1,) or ()).\n",
        "print(f\"Forward output for EstimatorQNN1: {estimator_qnn_forward.shape}\")\n",
        "\n",
        "# Print the shape of the output from the new multi-observable EstimatorQNN (estimator_qnn2),\n",
        "# which returns a vector of expectation values (one per observable).\n",
        "print(f\"Forward output for EstimatorQNN2: {estimator_qnn_forward2.shape}\")\n",
        "\n",
        "# Print the shape of the weight gradients for the original (single-output) EstimatorQNN.\n",
        "# This will be a 1D array with shape (num_weights,).\n",
        "print(f\"Backward output for EstimatorQNN1: {estimator_qnn_weight_grad.shape}\")\n",
        "\n",
        "# Print the shape of the weight gradients for the multi-output EstimatorQNN.\n",
        "# This will be a 2D array: (num_observables, num_weights).\n",
        "print(f\"Backward output for EstimatorQNN2: {estimator_qnn_weight_grad2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788ec9f1"
      },
      "source": [
        "### 6.2. `SamplerQNN` with custom `interpret`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378ef3ba"
      },
      "source": [
        "One common `interpret` method for `SamplerQNN` is the `parity` function, which allows it to perform binary classification. As explained in the instantiation section, using interpret functions will modify the output shape of the forward and backward passes. In the case of the parity interpret function, `output_shape` is fixed to `2`. Therefore, the expected forward and weight gradient shapes are `(batch_size, 2)` and `(batch_size, 2, num_weights)`, respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8afvPJXPzbtO"
      },
      "outputs": [],
      "source": [
        "# Define a lambda function named 'parity' that takes an integer x (representing a bitstring as an integer),\n",
        "# converts it to binary format, counts the number of '1's, and returns 0 if even, 1 if odd.\n",
        "# This function will be used to map bitstring outputs (e.g., '00', '01', '10', '11') to their parity class.\n",
        "# Example:  '00' -> 0 (even), '01' -> 1 (odd), '11' -> 0 (even number of 1s = even parity)\n",
        "parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "# Define the number of output classes (0 = even parity, 1 = odd parity), so total of 2 classes.\n",
        "output_shape = 2\n",
        "\n",
        "# Create a new Sampler-based Quantum Neural Network (SamplerQNN) with custom output interpretation.\n",
        "# Parameters:\n",
        "# - circuit: the quantum circuit (qc2) with symbolic input and weight parameters.\n",
        "# - input_params: parameters used to encode classical input features (inputs2).\n",
        "# - weight_params: trainable parameters (weights2) that the model will learn during training.\n",
        "# - interpret: a function that maps raw measurement outcomes (bitstrings) to class labels using parity.\n",
        "# - output_shape: the number of unique output classes returned by the interpret function (here: 0 and 1).\n",
        "# - sampler: the backend used to simulate the quantum circuit and return output probabilities.\n",
        "sampler_qnn2 = SamplerQNN(\n",
        "    circuit=qc2,\n",
        "    input_params=inputs2,\n",
        "    weight_params=weights2,\n",
        "    interpret=parity,\n",
        "    output_shape=output_shape,\n",
        "    sampler=sampler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSEDG7oozdFP"
      },
      "outputs": [],
      "source": [
        "# Perform a forward pass through the second SamplerQNN (sampler_qnn2),\n",
        "# which uses a custom interpret function (parity) to group raw quantum measurement outcomes\n",
        "# into one of two output classes: 0 (even parity) or 1 (odd parity).\n",
        "# The output is a probability distribution over these two interpreted classes.\n",
        "sampler_qnn_forward2 = sampler_qnn2.forward(sampler_qnn_input, sampler_qnn_weights)\n",
        "\n",
        "# Compute the gradients of the output probabilities from sampler_qnn2\n",
        "# with respect to both input and weight parameters.\n",
        "# Returns:\n",
        "# - sampler_qnn_input_grad2: ∂output/∂input for each interpreted class.\n",
        "# - sampler_qnn_weight_grad2: ∂output/∂weights for each interpreted class.\n",
        "sampler_qnn_input_grad2, sampler_qnn_weight_grad2 = sampler_qnn2.backward(\n",
        "    sampler_qnn_input,\n",
        "    sampler_qnn_weights\n",
        ")\n",
        "\n",
        "# Print the shape of the forward pass output from the original SamplerQNN (sampler_qnn),\n",
        "# which returns the full output distribution over all raw measurement outcomes (e.g., 4 outcomes for 2 qubits).\n",
        "print(f\"Forward output for SamplerQNN1: {sampler_qnn_forward.shape}\")\n",
        "\n",
        "# Print the shape of the forward pass output from sampler_qnn2,\n",
        "# which returns a reduced output distribution over parity classes (e.g., shape = (2,) for 2 classes).\n",
        "print(f\"Forward output for SamplerQNN2: {sampler_qnn_forward2.shape}\")\n",
        "\n",
        "# Print the shape of the weight gradients from the original SamplerQNN,\n",
        "# which correspond to the full output space (without interpretation).\n",
        "print(f\"Backward output for SamplerQNN1: {sampler_qnn_weight_grad.shape}\")\n",
        "\n",
        "# Print the shape of the weight gradients from sampler_qnn2,\n",
        "# which are reduced to the interpreted class space (e.g., shape = (2, num_weights) for 2 classes).\n",
        "print(f\"Backward output for SamplerQNN2: {sampler_qnn_weight_grad2.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intense-ecology"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Neural Network Classifier & Regressor\n",
        "\n",
        "In this part, we explore how to use Qiskit Machine Learning’s high-level interfaces for supervised learning: the `NeuralNetworkClassifier` and `NeuralNetworkRegressor`. These classes serve as wrappers around (quantum or classical) neural networks, allowing them to be easily integrated into standard machine learning workflows such as training, evaluation, and prediction. They are compatible with scikit-learn-like APIs and support the use of quantum neural networks (`EstimatorQNN`, `SamplerQNN`, etc.) as underlying models.\n",
        "\n",
        "Both the classifier and regressor require a `NeuralNetwork` object as input, which determines the model's architecture and computational behavior. The `NeuralNetworkClassifier` is designed for categorical prediction tasks (e.g., binary or multi-class classification), while the `NeuralNetworkRegressor` is intended for continuous output tasks (e.g., predicting real-valued labels).\n",
        "\n",
        "To simplify usage and accelerate prototyping, Qiskit also provides two pre-configured convenience classes:\n",
        "\n",
        "* **Variational Quantum Classifier (VQC)** – a ready-to-use quantum model for classification, which combines a feature map, ansatz, optimizer, and training loop.\n",
        "* **Variational Quantum Regressor (VQR)** – similar to VQC but optimized for regression problems.\n",
        "\n",
        "We will have two main examples — **Classification** and **Regression** —each further broken down to show different modeling strategies using both low-level and high-level tools.\n",
        "\n",
        "#### Classification\n",
        "\n",
        "* **Classification with an `EstimatorQNN`**: Demonstrates how to use a quantum neural network that returns expectation values as logits for classification.\n",
        "* **Classification with a `SamplerQNN`**: Illustrates classification using output probability distributions derived from measurement outcomes.\n",
        "* **Variational Quantum Classifier (VQC)**: Shows how to use Qiskit’s VQC class for a fully encapsulated variational classification pipeline.\n",
        "\n",
        "#### Regression\n",
        "\n",
        "* **Regression with an `EstimatorQNN`**: Implements regression tasks using expectation values from a quantum circuit.\n",
        "* **Variational Quantum Regressor (VQR)**: Provides an end-to-end quantum regression model using a variational approach, ready to train on real-valued targets.\n",
        "\n",
        "We will cover model initialization, training, prediction, and performance evaluation. This structure offers a practical overview of how to use Qiskit’s neural network components to solve both classification and regression tasks with quantum-enhanced machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "AFQAeScA1Q6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import Parameter\n",
        "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
        "from qiskit_machine_learning.optimizers import COBYLA, L_BFGS_B\n",
        "from qiskit_machine_learning.utils import algorithm_globals\n",
        "\n",
        "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n",
        "from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n",
        "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
        "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
        "\n",
        "algorithm_globals.random_seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compact-divide"
      },
      "source": [
        "## 7.1. Classification\n",
        "\n",
        "We prepare a simple classification dataset to illustrate the following algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt8HhbIs1Ux3"
      },
      "outputs": [],
      "source": [
        "# Set the number of input features (dimensions) and number of samples to generate.\n",
        "num_inputs = 2\n",
        "num_samples = 20\n",
        "\n",
        "# Generate a 20x2 array of input features X with values uniformly distributed in the range [-1, 1].\n",
        "# This simulates a 2D dataset for binary classification.\n",
        "X = 2 * algorithm_globals.random.random([num_samples, num_inputs]) - 1\n",
        "\n",
        "# Create binary labels (0 or 1) based on the sum of the two input features.\n",
        "# If the sum is non-negative, label is 1; otherwise, label is 0.\n",
        "y01 = 1 * (np.sum(X, axis=1) >= 0)  # Labels in {0, 1}\n",
        "\n",
        "# Convert labels from {0, 1} to {-1, +1} for algorithms that expect this format.\n",
        "y = 2 * y01 - 1  # Labels in {-1, +1}\n",
        "\n",
        "# Initialize a one-hot encoded label matrix of shape (20, 2).\n",
        "y_one_hot = np.zeros((num_samples, 2))\n",
        "\n",
        "# Populate the one-hot encoded matrix based on the binary labels.\n",
        "for i in range(num_samples):\n",
        "    y_one_hot[i, y01[i]] = 1\n",
        "\n",
        "# Plot each data point in 2D space using a different color based on its label.\n",
        "# Blue circles (\"bo\") for label +1, green circles (\"go\") for label -1.\n",
        "for x, y_target in zip(X, y):\n",
        "    if y_target == 1:\n",
        "        plt.plot(x[0], x[1], \"bo\")  # Class +1 (blue)\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], \"go\")  # Class -1 (green)\n",
        "\n",
        "# Plot a black dashed decision boundary line (y = -x) to visually separate the two classes.\n",
        "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
        "\n",
        "# Display the scatter plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "religious-history"
      },
      "source": [
        "### 7.1.2. Classification with an `EstimatorQNN`\n",
        "\n",
        "First we show how an `EstimatorQNN` can be used for classification within a `NeuralNetworkClassifier`. In this context, the `EstimatorQNN` is expected to return one-dimensional output in $[-1, +1]$. This only works for binary classification and we assign the two classes to $\\{-1, +1\\}$. To simplify the composition of parameterized quantum circuit from a feature map and an ansatz we can use the `QNNCircuit` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JoKDAsd1XeQ"
      },
      "outputs": [],
      "source": [
        "# construct QNN with the QNNCircuit's default ZZFeatureMap feature map and RealAmplitudes ansatz.\n",
        "qc = QNNCircuit(num_qubits=2)\n",
        "qc.draw(\"mpl\", style=\"clifford\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formed-animal"
      },
      "source": [
        "Create a quantum neural network.  As we are performing a local statevector simulation, we will set the `estimator` parameter from `qiskit.primitives.StatevectorEstimator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZg9sVhs1Zo6"
      },
      "outputs": [],
      "source": [
        "# Import the StatevectorEstimator primitive from Qiskit.\n",
        "# This estimator simulates quantum circuits using ideal statevector simulation,\n",
        "# allowing precise and noise-free computation of expectation values.\n",
        "from qiskit.primitives import StatevectorEstimator as Estimator\n",
        "\n",
        "# Create an instance of the StatevectorEstimator.\n",
        "# This object will be used to evaluate expectation values of observables in quantum circuits.\n",
        "estimator = Estimator()\n",
        "\n",
        "# Create an Estimator-based Quantum Neural Network (EstimatorQNN).\n",
        "# - 'circuit' is a parameterized quantum circuit (e.g., a variational model or feature map).\n",
        "# - 'estimator' is the simulation backend used to evaluate the quantum circuit.\n",
        "# Note: Since no input_params, weight_params, or observables are specified here,\n",
        "# they will be inferred or set to default behavior if the circuit is compatible.\n",
        "estimator_qnn = EstimatorQNN(circuit=qc, estimator=estimator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ov5PLFl1bSf"
      },
      "outputs": [],
      "source": [
        "# QNN maps inputs to [-1, +1]\n",
        "estimator_qnn.forward(X[0, :], algorithm_globals.random.random(estimator_qnn.num_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stone-holiday"
      },
      "source": [
        "We will add a callback function called `callback_graph`. This will be called for each iteration of the optimizer and will be passed two parameters: the current weights and the value of the objective function at those weights. For our function, we append the value of the objective function to an array so we can plot iteration versus objective function value and update the graph with each iteration. However, you can do whatever you want with a callback function as long as it gets the two parameters mentioned passed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6zUeLKuf1dsf"
      },
      "outputs": [],
      "source": [
        "# Define a callback function to be used during training with .fit().\n",
        "# This function is called at each iteration of the optimizer to visualize the training progress.\n",
        "\n",
        "def callback_graph(weights, obj_func_eval):\n",
        "    # Clear the current output to update the plot in-place (used in Jupyter notebooks).\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Append the current value of the objective function (e.g., loss or cost) to a global list.\n",
        "    # This allows us to track how the objective changes over time.\n",
        "    objective_func_vals.append(obj_func_eval)\n",
        "\n",
        "    # Set the plot title and axis labels.\n",
        "    plt.title(\"Objective function value against iteration\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Objective function value\")\n",
        "\n",
        "    # Plot the objective function values over all completed iterations.\n",
        "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
        "\n",
        "    # Display the updated plot.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "O_WDAelf1fTC"
      },
      "outputs": [],
      "source": [
        "# Construct a neural network-based classifier using Qiskit's NeuralNetworkClassifier.\n",
        "# This wrapper allows a quantum neural network (QNN) to be trained for classification tasks\n",
        "# using a scikit-learn-style interface.\n",
        "\n",
        "estimator_classifier = NeuralNetworkClassifier(\n",
        "    estimator_qnn,                    # The quantum neural network model to be used (EstimatorQNN in this case)\n",
        "    optimizer=COBYLA(maxiter=60),     # Classical optimizer (COBYLA) with a maximum of 60 iterations\n",
        "    callback=callback_graph           # Callback function to visualize training progress (live plot of the loss)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDF87QI-1gfb"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store the objective function values during training.\n",
        "# This will be populated by the callback function (callback_graph) at each optimization step.\n",
        "objective_func_vals = []\n",
        "\n",
        "# Optionally enlarge the default plot size for clearer visualization of the training graph.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train (fit) the neural network classifier on the dataset (features X and labels y).\n",
        "# During training, the optimizer will update weights to minimize the objective function,\n",
        "# and the callback will update the live plot after each iteration.\n",
        "estimator_classifier.fit(X, y)\n",
        "\n",
        "# Restore the default plot size for future plots after training is complete.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the trained classifier on the same dataset by computing accuracy.\n",
        "# This returns the proportion of correctly predicted labels.\n",
        "estimator_classifier.score(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHNDewm71pTJ"
      },
      "outputs": [],
      "source": [
        "# Use the trained estimator_classifier to predict class labels for all data points in X.\n",
        "# The result y_predict contains the predicted labels for each sample.\n",
        "y_predict = estimator_classifier.predict(X)\n",
        "\n",
        "# Start visualizing the classification results.\n",
        "# Red circles will highlight data points that were misclassified.\n",
        "\n",
        "# Loop through all input samples, their true labels (y_target), and predicted labels (y_p).\n",
        "for x, y_target, y_p in zip(X, y, y_predict):\n",
        "    # Plot class +1 examples (label = 1) as blue circles.\n",
        "    if y_target == 1:\n",
        "        plt.plot(x[0], x[1], \"bo\")\n",
        "    # Plot class -1 examples (label = -1) as green circles.\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], \"go\")\n",
        "\n",
        "    # If the predicted label is incorrect, draw a red ring around the misclassified point.\n",
        "    if y_target != y_p:\n",
        "        plt.scatter(\n",
        "            x[0], x[1],                # Coordinates of the misclassified point\n",
        "            s=200,                     # Marker size\n",
        "            facecolors=\"none\",         # Transparent fill\n",
        "            edgecolors=\"r\",            # Red border\n",
        "            linewidths=2               # Line thickness\n",
        "        )\n",
        "\n",
        "# Plot the decision boundary line (y = -x) to separate the two classes visually.\n",
        "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
        "\n",
        "# Display the final plot.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "japanese-seattle"
      },
      "source": [
        "Now, when the model is trained, we can explore the weights of the neural network. Please note, the number of weights is defined by ansatz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t8LiIcC1sJ_"
      },
      "outputs": [],
      "source": [
        "# Retrieve the trained weights from the estimator-based quantum neural network classifier.\n",
        "# These are the optimized parameters found by the classical optimizer (e.g., COBYLA) during training.\n",
        "# The weights correspond to the parameterized gates in the quantum circuit (e.g., Ry, Rx).\n",
        "estimator_classifier.weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "determined-standing"
      },
      "source": [
        "### 7.1.3. Classification with a `SamplerQNN`\n",
        "\n",
        "Next we show how a `SamplerQNN` can be used for classification within a `NeuralNetworkClassifier`. In this context, the `SamplerQNN` is expected to return $d$-dimensional probability vector as output, where $d$ denotes the number of classes.\n",
        "The underlying `Sampler` primitive returns quasi-distributions of bit strings and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping. Again we can use the `QNNCircuit` class to set up a parameterized quantum circuit from a feature map and ansatz of our choice. Please note that, once again, we are setting the `StatevectorSampler` instance from `qiskit.primitives` to the QNN and relying on `statevector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXR_hkxM1uuf"
      },
      "outputs": [],
      "source": [
        "# Construct a parameterized quantum circuit for use in a quantum neural network.\n",
        "# This circuit combines:\n",
        "# - A default ZZFeatureMap (automatically included by QNNCircuit) to encode classical input features.\n",
        "# - A custom ansatz: RealAmplitudes with a specified number of qubits and one repetition.\n",
        "\n",
        "# RealAmplitudes is a commonly used variational ansatz with parameterized Ry rotations and entangling CX gates.\n",
        "# It defines the trainable part of the quantum circuit.\n",
        "\n",
        "qc = QNNCircuit(ansatz=RealAmplitudes(num_inputs, reps=1))\n",
        "\n",
        "# Visualize the constructed quantum circuit using matplotlib and the 'clifford' style.\n",
        "# This shows the sequence of quantum gates applied to each qubit.\n",
        "qc.draw(\"mpl\", style=\"clifford\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iGYO0-RR1wHt"
      },
      "outputs": [],
      "source": [
        "# Define a function that maps a bitstring (represented as an integer `x`) to its parity.\n",
        "# The parity is calculated by:\n",
        "# - Converting the integer to its binary representation using format(x, \"b\")\n",
        "# - Counting the number of 1s in the bitstring\n",
        "# - Returning 0 if the number of 1s is even, or 1 if it is odd\n",
        "# This is commonly used to reduce quantum measurement outcomes (bitstrings) to two classes (even or odd parity).\n",
        "def parity(x):\n",
        "    return \"{:b}\".format(x).count(\"1\") % 2\n",
        "\n",
        "# Define the output shape of the neural network as 2, which corresponds to the number of classes:\n",
        "# - Class 0: even parity\n",
        "# - Class 1: odd parity\n",
        "output_shape = 2  # Number of possible output labels after applying the parity interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iettgSRH1xMd"
      },
      "outputs": [],
      "source": [
        "# Import the StatevectorSampler from Qiskit's primitives module.\n",
        "# This sampler simulates quantum circuits using ideal (noise-free) statevector simulation,\n",
        "# and returns exact output probability distributions.\n",
        "from qiskit.primitives import StatevectorSampler as Sampler\n",
        "\n",
        "# Instantiate the statevector-based sampler.\n",
        "# This backend will be used to evaluate the quantum circuit during forward and backward passes.\n",
        "sampler = Sampler()\n",
        "\n",
        "# Construct a quantum neural network using the SamplerQNN class.\n",
        "# Parameters:\n",
        "# - circuit: the parameterized quantum circuit (`qc`) that encodes inputs and contains trainable weights.\n",
        "# - interpret: a function (`parity`) that maps raw bitstring outputs (e.g., '00', '11') to class labels (0 or 1).\n",
        "# - output_shape: the number of output classes (2 in this case, for even and odd parity).\n",
        "# - sampler: the backend used to simulate circuit execution and obtain output probabilities.\n",
        "\n",
        "sampler_qnn = SamplerQNN(\n",
        "    circuit=qc,\n",
        "    interpret=parity,\n",
        "    output_shape=output_shape,\n",
        "    sampler=sampler,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "gaK08jgv1ydd"
      },
      "outputs": [],
      "source": [
        "# Construct a quantum neural network classifier using the SamplerQNN.\n",
        "# NeuralNetworkClassifier is a scikit-learn-style wrapper that enables training and evaluation\n",
        "# of quantum (or classical) neural networks for classification tasks.\n",
        "\n",
        "sampler_classifier = NeuralNetworkClassifier(\n",
        "    neural_network=sampler_qnn,         # The quantum neural network based on output probabilities\n",
        "    optimizer=COBYLA(maxiter=30),       # Classical optimizer (COBYLA) with a maximum of 30 iterations\n",
        "    callback=callback_graph             # Callback function used to visualize training progress (e.g., loss plot)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX8Mc_MU1zqj"
      },
      "outputs": [],
      "source": [
        "# Create an empty list to store objective function values during training.\n",
        "# This list will be updated at each optimizer iteration by the callback function (callback_graph),\n",
        "# allowing for real-time visualization of the training progress.\n",
        "objective_func_vals = []\n",
        "\n",
        "# Set a larger default figure size for matplotlib plots to make the training graph easier to read.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train (fit) the sampler-based quantum classifier on the dataset.\n",
        "# - X: input feature vectors\n",
        "# - y01: class labels (in {0, 1}, required for classification tasks)\n",
        "# During training, the optimizer updates the weights to minimize the loss function.\n",
        "# The callback function updates the loss plot live at each iteration.\n",
        "sampler_classifier.fit(X, y01)\n",
        "\n",
        "# Restore the default figure size for future plots.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the trained classifier on the same dataset.\n",
        "# Returns the accuracy score: the proportion of correctly predicted labels.\n",
        "sampler_classifier.score(X, y01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sePaYkpb11UR"
      },
      "outputs": [],
      "source": [
        "# Use the trained sampler-based quantum classifier to predict class labels for all samples in X.\n",
        "# The result is an array of predicted labels (y_predict) corresponding to the input data.\n",
        "y_predict = sampler_classifier.predict(X)\n",
        "\n",
        "# Start plotting the classification results.\n",
        "# Data points are shown in blue or green based on their true class.\n",
        "# A red ring is used to highlight misclassified points.\n",
        "\n",
        "# Loop over the feature vectors (x), true labels (y_target), and predicted labels (y_p).\n",
        "for x, y_target, y_p in zip(X, y01, y_predict):\n",
        "    # Plot samples with true label 1 as blue circles (\"bo\").\n",
        "    if y_target == 1:\n",
        "        plt.plot(x[0], x[1], \"bo\")\n",
        "    # Plot samples with true label 0 as green circles (\"go\").\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], \"go\")\n",
        "\n",
        "    # If the predicted label does not match the true label, highlight the point with a red ring.\n",
        "    if y_target != y_p:\n",
        "        plt.scatter(\n",
        "            x[0], x[1],                # Coordinates of the misclassified point\n",
        "            s=200,                     # Size of the ring\n",
        "            facecolors=\"none\",         # No fill color\n",
        "            edgecolors=\"r\",            # Red border\n",
        "            linewidths=2               # Thickness of the ring outline\n",
        "        )\n",
        "\n",
        "# Plot the decision boundary line (y = -x) for visual reference (used when generating the dataset).\n",
        "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
        "\n",
        "# Display the final visualization.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assisted-individual"
      },
      "source": [
        "Again, once the model is trained we can take a look at the weights. As we set `reps=1` explicitly in our ansatz, we can see less parameters than in the previous model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb5d8Be617N3"
      },
      "outputs": [],
      "source": [
        "# Retrieve the optimized weights from the trained sampler-based quantum neural network classifier.\n",
        "# These weights correspond to the parameterized gates in the ansatz circuit,\n",
        "# and were adjusted during training using the optimizer (e.g., COBYLA).\n",
        "\n",
        "sampler_classifier.weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "champion-approval"
      },
      "source": [
        "### 7.1.4. Variational Quantum Classifier (`VQC`)\n",
        "\n",
        "The `VQC` is a special variant of the `NeuralNetworkClassifier` with a `SamplerQNN`. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the `CrossEntropyLoss` function that expects labels given in one-hot encoded format and will return predictions in that format too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o5RU9za2KMK"
      },
      "outputs": [],
      "source": [
        "# Construct a feature map that encodes classical input data into a quantum state.\n",
        "# - ZZFeatureMap uses entangling ZZ interactions to map data non-linearly into the quantum Hilbert space.\n",
        "# - The number of qubits is set equal to the number of input features.\n",
        "feature_map = ZZFeatureMap(num_inputs)\n",
        "\n",
        "# Define the variational ansatz (a parameterized quantum circuit) that will be trained.\n",
        "# - RealAmplitudes applies Ry rotations and entangling CX gates.\n",
        "# - reps=1 means a single layer of rotations + entanglements is used.\n",
        "ansatz = RealAmplitudes(num_inputs, reps=1)\n",
        "\n",
        "# Construct a Variational Quantum Classifier (VQC), a prebuilt quantum machine learning model.\n",
        "# Parameters:\n",
        "# - feature_map: the circuit that encodes classical data into quantum states.\n",
        "# - ansatz: the parameterized circuit with trainable weights.\n",
        "# - loss: the objective function used to guide training (cross-entropy for classification).\n",
        "# - optimizer: COBYLA optimizer with a maximum of 30 iterations to train the ansatz parameters.\n",
        "# - callback: function called after each optimization step (e.g., to visualize loss).\n",
        "# - sampler: backend used to simulate the quantum circuit and return measurement probabilities.\n",
        "vqc = VQC(\n",
        "    feature_map=feature_map,\n",
        "    ansatz=ansatz,\n",
        "    loss=\"cross_entropy\",\n",
        "    optimizer=COBYLA(maxiter=30),\n",
        "    callback=callback_graph,\n",
        "    sampler=sampler,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwIw4DAc2Lcr"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the values of the objective (loss) function during training.\n",
        "# This list will be updated at each iteration by the callback function (callback_graph).\n",
        "objective_func_vals = []\n",
        "\n",
        "# Enlarge the default plot size for better visibility of the training progress graph.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train (fit) the Variational Quantum Classifier (VQC) using the provided training data.\n",
        "# - X: input features (real-valued vectors)\n",
        "# - y_one_hot: one-hot encoded class labels (required for cross-entropy loss)\n",
        "# During training, the optimizer updates the parameters of the ansatz circuit to minimize loss.\n",
        "# The callback function displays a live plot of the objective function value over iterations.\n",
        "vqc.fit(X, y_one_hot)\n",
        "\n",
        "# Reset the plot size to the default after training is complete.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the trained VQC model on the training data by calculating the classification accuracy.\n",
        "# - Returns the proportion of correctly predicted samples.\n",
        "vqc.score(X, y_one_hot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STDwerk32NI1"
      },
      "outputs": [],
      "source": [
        "# Use the trained Variational Quantum Classifier (VQC) to predict labels for all input samples.\n",
        "# y_predict will contain the predicted class labels in one-hot encoded format (e.g., [1, 0] or [0, 1]).\n",
        "y_predict = vqc.predict(X)\n",
        "\n",
        "# Begin plotting the classification results.\n",
        "# Blue and green markers represent the two true classes.\n",
        "# Red rings will highlight samples that were misclassified.\n",
        "\n",
        "# Iterate over all input samples, their true labels (y_target), and predicted labels (y_p).\n",
        "for x, y_target, y_p in zip(X, y_one_hot, y_predict):\n",
        "    # If the true class is class 0 (i.e., y_target = [1, 0]), plot it as a blue circle.\n",
        "    if y_target[0] == 1:\n",
        "        plt.plot(x[0], x[1], \"bo\")\n",
        "    # Otherwise (y_target = [0, 1]), plot it as a green circle.\n",
        "    else:\n",
        "        plt.plot(x[0], x[1], \"go\")\n",
        "\n",
        "    # If the prediction does not match the true label, highlight it with a red ring.\n",
        "    if not np.all(y_target == y_p):\n",
        "        plt.scatter(\n",
        "            x[0], x[1],                # Coordinates of the misclassified point\n",
        "            s=200,                     # Size of the red ring\n",
        "            facecolors=\"none\",         # Transparent fill\n",
        "            edgecolors=\"r\",            # Red border\n",
        "            linewidths=2               # Border thickness\n",
        "        )\n",
        "\n",
        "# Plot the decision boundary (y = -x), which visually separates the two classes in the original data.\n",
        "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
        "\n",
        "# Display the final classification result plot.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grave-testament"
      },
      "source": [
        "### 7.1.5. Multiple classes with VQC\n",
        "In this section we generate an artificial dataset that contains samples of three classes and show how to train a model to classify this dataset. This example shows how to tackle more interesting problems in machine learning. Of course, for a sake of short training time we prepare a tiny dataset. We employ `make_classification` from SciKit-Learn to generate a dataset. There 10 samples in the dataset, 2 features, that means we can still have a nice plot of the dataset, as well as no redundant features, these are features are generated as a combinations of the other features. Also, we have 3 different classes in the dataset, each classes one kind of centroid and we set class separation to `2.0`, a slight increase from the default value of `1.0` to ease the classification problem.\n",
        "\n",
        "Once the dataset is generated we scale the features into the range `[0, 1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Yy9QKDoi2Rcf"
      },
      "outputs": [],
      "source": [
        "# Import dataset generation and preprocessing utilities from scikit-learn.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Generate a synthetic classification dataset using scikit-learn's make_classification.\n",
        "# Parameters:\n",
        "# - n_samples=10: generate 10 data points\n",
        "# - n_features=2: each data point has 2 features (suitable for visualization or 2-qubit circuits)\n",
        "# - n_classes=3: create a 3-class classification problem\n",
        "# - n_redundant=0: no redundant (correlated) features\n",
        "# - n_clusters_per_class=1: each class is centered around one cluster\n",
        "# - class_sep=2.0: controls the distance between classes; higher = more separable\n",
        "# - random_state=algorithm_globals.random_seed: ensures reproducibility with a fixed seed\n",
        "X, y = make_classification(\n",
        "    n_samples=10,\n",
        "    n_features=2,\n",
        "    n_classes=3,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=2.0,\n",
        "    random_state=algorithm_globals.random_seed,\n",
        ")\n",
        "\n",
        "# Normalize the feature values to the range [0, 1] using MinMaxScaler.\n",
        "# This is often necessary for compatibility with quantum circuits,\n",
        "# which expect inputs to fall within a certain bounded range (e.g., for rotation angles).\n",
        "X = MinMaxScaler().fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forced-disclosure"
      },
      "source": [
        "Let's see how our dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ4HpkM92T20"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot of the dataset to visualize the distribution of input features and classes.\n",
        "# - X[:, 0] are the x-coordinates (first feature)\n",
        "# - X[:, 1] are the y-coordinates (second feature)\n",
        "# - c=y assigns a different color to each point based on its class label (0, 1, or 2)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deadly-response"
      },
      "source": [
        "We also transform labels and make them categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl1UC1RM2WIt"
      },
      "outputs": [],
      "source": [
        "# Create an empty NumPy array `y_cat` with the same shape as `y`, but with string (str) data type.\n",
        "# This array will store categorical string labels instead of numeric class labels.\n",
        "y_cat = np.empty(y.shape, dtype=str)\n",
        "\n",
        "# Assign the label \"A\" to all elements in `y_cat` where the original class label is 0.\n",
        "y_cat[y == 0] = \"A\"\n",
        "\n",
        "# Assign the label \"B\" to all elements in `y_cat` where the original class label is 1.\n",
        "y_cat[y == 1] = \"B\"\n",
        "\n",
        "# Assign the label \"C\" to all elements in `y_cat` where the original class label is 2.\n",
        "y_cat[y == 2] = \"C\"\n",
        "\n",
        "# Print the resulting array of categorical labels.\n",
        "print(y_cat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructional-headquarters"
      },
      "source": [
        "We create an instance of `VQC` similar to the previous example, but in this case we pass a minimal set of parameters. Instead of feature map and ansatz we pass just the number of qubits that is equal to the number of features in the dataset, an optimizer with a low number of iteration to reduce training time, a quantum instance, and a callback to observe progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgzhx4gY2YOO"
      },
      "outputs": [],
      "source": [
        "# Create a Variational Quantum Classifier (VQC) using default components and basic configuration.\n",
        "# VQC is a high-level quantum machine learning model provided by Qiskit for classification tasks.\n",
        "\n",
        "vqc = VQC(\n",
        "    num_qubits=2,                      # Number of qubits (matches the number of input features)\n",
        "                                       # This determines the size of the quantum circuit (feature map & ansatz)\n",
        "    optimizer=COBYLA(maxiter=30),      # Classical optimizer (COBYLA) with a maximum of 30 iterations\n",
        "                                       # Used to minimize the loss by adjusting parameters in the ansatz\n",
        "    callback=callback_graph,           # Callback function that visualizes the objective function value live\n",
        "    sampler=sampler,                   # Sampler backend (e.g., StatevectorSampler) to evaluate circuit outcomes\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "proper-bookmark"
      },
      "source": [
        "Start the training process in the same way as in previous examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PVL9Jqp2aSv"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store objective (loss) function values during optimization.\n",
        "# This will be populated by the callback function after each optimizer iteration.\n",
        "objective_func_vals = []\n",
        "\n",
        "# Increase the default plot size to improve visibility of the loss curve during training.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Fit (train) the Variational Quantum Classifier (VQC) on the dataset.\n",
        "# - X: input features (2D vectors, normalized)\n",
        "# - y_cat: categorical class labels (e.g., \"A\", \"B\", \"C\")\n",
        "# During training, the VQC uses the sampler and optimizer to adjust the ansatz parameters to minimize loss.\n",
        "vqc.fit(X, y_cat)\n",
        "\n",
        "# Restore the default figure size for future plots.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the trained VQC on the training data and return the classification accuracy.\n",
        "# Accuracy is calculated as the percentage of correctly predicted labels.\n",
        "vqc.score(X, y_cat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weighted-renaissance"
      },
      "source": [
        "Despite we had the low number of iterations, we achieved quite a good score. Let see the output of the `predict` method and compare the output with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RtHi78j2c7-"
      },
      "outputs": [],
      "source": [
        "# Use the trained Variational Quantum Classifier (VQC) to predict class labels for the input data X.\n",
        "# The predicted labels will be in the same categorical format as the training labels (e.g., \"A\", \"B\", \"C\").\n",
        "predict = vqc.predict(X)\n",
        "\n",
        "# Print the predicted class labels for all input samples.\n",
        "print(f\"Predicted labels: {predict}\")\n",
        "\n",
        "# Print the corresponding ground truth (actual) class labels for comparison.\n",
        "print(f\"Ground truth:     {y_cat}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guided-secret"
      },
      "source": [
        "## 7.2. Regression\n",
        "\n",
        "We prepare a simple regression dataset to illustrate the following algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLyvO0dt2gFW"
      },
      "outputs": [],
      "source": [
        "# Define the number of training samples to generate.\n",
        "num_samples = 20\n",
        "\n",
        "# Define the amplitude of noise to add to the training labels.\n",
        "eps = 0.2\n",
        "\n",
        "# Set the lower and upper bounds of the input domain.\n",
        "lb, ub = -np.pi, np.pi\n",
        "\n",
        "# Create a set of 50 evenly spaced input values from -π to π for plotting the true sine function.\n",
        "X_ = np.linspace(lb, ub, num=50).reshape(50, 1)\n",
        "\n",
        "# Define the target function: sine of the input.\n",
        "f = lambda x: np.sin(x)\n",
        "\n",
        "# Generate `num_samples` random input values uniformly sampled from the interval [-π, π].\n",
        "X = (ub - lb) * algorithm_globals.random.random([num_samples, 1]) + lb\n",
        "\n",
        "# Compute noisy target values by evaluating the sine function and adding random noise in the range [-eps, +eps].\n",
        "y = f(X[:, 0]) + eps * (2 * algorithm_globals.random.random(num_samples) - 1)\n",
        "\n",
        "# Plot the true sine function as a red dashed curve.\n",
        "plt.plot(X_, f(X_), \"r--\")\n",
        "\n",
        "# Plot the noisy sampled data points as blue circles.\n",
        "plt.plot(X, y, \"bo\")\n",
        "\n",
        "# Display the final plot.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "talented-capitol"
      },
      "source": [
        "### 7.2.1. Regression with an `EstimatorQNN`\n",
        "\n",
        "Here we restrict to regression with an `EstimatorQNN` that returns values in $[-1, +1]$. More complex and also multi-dimensional models could be constructed, also based on `SamplerQNN` but that exceeds the scope of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2sOf8ro2ihS"
      },
      "outputs": [],
      "source": [
        "# Define a symbolic parameter 'x' for the input feature.\n",
        "param_x = Parameter(\"x\")\n",
        "\n",
        "# Construct a simple 1-qubit quantum feature map.\n",
        "# This circuit will encode classical input 'x' into the quantum state using a Ry rotation.\n",
        "feature_map = QuantumCircuit(1, name=\"fm\")\n",
        "feature_map.ry(param_x, 0)  # Apply a Ry gate to qubit 0 using parameter x\n",
        "\n",
        "# Define a symbolic parameter 'y' for the trainable weight (used in the ansatz).\n",
        "param_y = Parameter(\"y\")\n",
        "\n",
        "# Construct a simple 1-qubit variational ansatz circuit.\n",
        "# This will represent the trainable part of the model using a Ry rotation.\n",
        "ansatz = QuantumCircuit(1, name=\"vf\")\n",
        "ansatz.ry(param_y, 0)  # Apply a Ry gate to qubit 0 using parameter y\n",
        "\n",
        "# Combine the feature map and ansatz into a complete quantum circuit using QNNCircuit.\n",
        "# QNNCircuit automates the stacking of feature encoding followed by the trainable ansatz.\n",
        "qc = QNNCircuit(feature_map=feature_map, ansatz=ansatz)\n",
        "\n",
        "# Construct an Estimator-based Quantum Neural Network for regression tasks.\n",
        "# The EstimatorQNN computes the expectation value of a default observable (Pauli Z)\n",
        "# from the output state of the circuit, which is used as the regression output.\n",
        "regression_estimator_qnn = EstimatorQNN(circuit=qc, estimator=estimator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "OFIGElSS2kAr"
      },
      "outputs": [],
      "source": [
        "# Construct a quantum regressor using Qiskit's NeuralNetworkRegressor class.\n",
        "# This wrapper integrates a quantum neural network (QNN) with a classical regression pipeline.\n",
        "\n",
        "regressor = NeuralNetworkRegressor(\n",
        "    neural_network=regression_estimator_qnn,  # The QNN model that outputs a scalar expectation value\n",
        "    loss=\"squared_error\",                     # Loss function: mean squared error, standard for regression tasks\n",
        "    optimizer=L_BFGS_B(maxiter=5),            # Classical optimizer: L-BFGS-B with a maximum of 5 iterations\n",
        "                                              # It uses gradient information to efficiently minimize the loss\n",
        "    callback=callback_graph,                  # Callback function to visualize training progress (loss per iteration)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLIRcp7N2mHH"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the objective (loss) values during training.\n",
        "# This list will be filled by the callback function at each optimizer step for plotting.\n",
        "objective_func_vals = []\n",
        "\n",
        "# Increase the plot size for better visibility of the training loss curve.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train (fit) the quantum regressor on the dataset.\n",
        "# - X: input features (e.g., angles for sine function)\n",
        "# - y: target values (e.g., noisy sine values)\n",
        "# The optimizer will update the parameters of the QNN to minimize the squared error loss.\n",
        "regressor.fit(X, y)\n",
        "\n",
        "# Reset the default plot size for future plots.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the performance of the trained regressor using the same dataset.\n",
        "# Returns the R² score (coefficient of determination), where 1.0 means perfect prediction.\n",
        "regressor.score(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BSMZjgI2pja"
      },
      "outputs": [],
      "source": [
        "# Plot the true (target) function using the clean sine function for reference.\n",
        "# This is shown as a red dashed line.\n",
        "plt.plot(X_, f(X_), \"r--\")\n",
        "\n",
        "# Plot the noisy training data points as blue circles.\n",
        "plt.plot(X, y, \"bo\")\n",
        "\n",
        "# Use the trained quantum regressor to predict output values over the smooth X_ grid.\n",
        "# This represents the model's learned approximation of the sine function.\n",
        "y_ = regressor.predict(X_)\n",
        "\n",
        "# Plot the regressor's predicted curve as a green solid line.\n",
        "plt.plot(X_, y_, \"g-\")\n",
        "\n",
        "# Display all the plots together for comparison: target function, noisy data, and model output.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "false-india"
      },
      "source": [
        "Similarly to the classification models, we can obtain an array of trained weights by querying a corresponding property of the model. In this model we have only one parameter defined as `param_y` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCpauL762ss3"
      },
      "outputs": [],
      "source": [
        "# Retrieve the trained weights from the regressor.\n",
        "# These weights correspond to the trainable parameters in the quantum circuit (ansatz),\n",
        "# optimized during the regression task to minimize the loss (squared error in this case).\n",
        "\n",
        "regressor.weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "offensive-legislation"
      },
      "source": [
        "### 7.2.2. Regression with the Variational Quantum Regressor (`VQR`)\n",
        "\n",
        "Similar to the `VQC` for classification, the `VQR` is a special variant of the `NeuralNetworkRegressor` with a `EstimatorQNN`. By default it considers the `L2Loss` function to minimize the mean squared error between predictions and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xRsRHeS2vBe"
      },
      "outputs": [],
      "source": [
        "# Construct a Variational Quantum Regressor (VQR) using Qiskit's prebuilt class.\n",
        "# The VQR is a high-level quantum regression model combining feature encoding, variational ansatz,\n",
        "# and classical optimization into a seamless training workflow.\n",
        "\n",
        "vqr = VQR(\n",
        "    feature_map=feature_map,             # Circuit to encode classical input features into quantum states\n",
        "    ansatz=ansatz,                       # Parameterized quantum circuit representing the trainable model\n",
        "    optimizer=L_BFGS_B(maxiter=5),      # Classical optimizer (L-BFGS-B) with max 5 iterations\n",
        "                                        # Efficient for minimizing smooth, continuous loss functions\n",
        "    callback=callback_graph,             # Function called after each optimizer iteration to track training progress\n",
        "    estimator=estimator,                 # Backend (e.g., StatevectorEstimator) used for circuit evaluation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uoLjwlC2wQt"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the values of the objective function during training.\n",
        "# The callback function will append the loss value at each iteration for visualization.\n",
        "objective_func_vals = []\n",
        "\n",
        "# Increase the figure size for better visibility of the training loss curve.\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Train (fit) the Variational Quantum Regressor (VQR) on the dataset.\n",
        "# - X: input feature vectors\n",
        "# - y: target continuous values\n",
        "# The optimizer adjusts the ansatz parameters to minimize the squared error or other loss function.\n",
        "vqr.fit(X, y)\n",
        "\n",
        "# Reset the matplotlib figure size to the default for future plots.\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
        "\n",
        "# Evaluate the trained VQR model by computing the R² score on the training data.\n",
        "# R² indicates how well the model's predictions match the true targets (1.0 is perfect).\n",
        "vqr.score(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJQxlpQI20-u"
      },
      "outputs": [],
      "source": [
        "# Plot the true target function (sine function) as a red dashed line for reference.\n",
        "plt.plot(X_, f(X_), \"r--\")\n",
        "\n",
        "# Plot the noisy training data points as blue circles.\n",
        "plt.plot(X, y, \"bo\")\n",
        "\n",
        "# Use the trained Variational Quantum Regressor (VQR) to predict values over the smooth input grid X_.\n",
        "y_ = vqr.predict(X_)\n",
        "\n",
        "# Plot the predicted regression curve as a green solid line.\n",
        "plt.plot(X_, y_, \"g-\")\n",
        "\n",
        "# Display the combined plot showing true function, noisy data, and model predictions.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LvGxb8-2piy"
      },
      "source": [
        "---\n",
        "\n",
        "## Questions 1\n",
        "\n",
        "1. What is the role of a quantum feature map in quantum machine learning, and why is the ZZFeatureMap commonly used?\n",
        "\n",
        "2. Explain the difference between the EstimatorQNN and SamplerQNN classes in Qiskit Machine Learning. When would you choose one over the other?\n",
        "\n",
        "3. Why do we combine a feature map and an ansatz circuit using the QNNCircuit wrapper?\n",
        "\n",
        "4. What is the purpose of the NeuralNetworkClassifier and NeuralNetworkRegressor wrappers? How do they facilitate the training process?\n",
        "\n",
        "5. Describe the significance of the classical optimizer (e.g., COBYLA, L_BFGS_B) in training quantum neural networks. Why can't quantum circuits be trained without classical optimization?\n",
        "\n",
        "6. In the regression example with the noisy sine wave, why do we use the squared error loss, and how does it influence training?\n",
        "\n",
        "7. What is the purpose of the callback function during training, and how does plotting the objective function values help in understanding the training progress?\n",
        "\n",
        "8. How does the NeuralNetworkClassifier handle multi-class classification differently from binary classification in the notebook?\n",
        "\n",
        "9. Explain how the EstimatorQNN outputs are used differently in classification and regression tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epHJg14t6JbT"
      },
      "source": [
        "---\n",
        "\n",
        "# Answers to Question Set 1\n",
        "\n",
        "### Write here:\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndjb1Rds5uPs"
      },
      "source": [
        "## Questions 2\n",
        "\n",
        "1. What is the purpose of the `Parameter` and `ParameterVector` classes in Qiskit circuits? How are they used in parameterized quantum circuits?\n",
        "\n",
        "2. How does the `QuantumCircuit.draw(\"mpl\", style=\"clifford\")` method work, and why is it useful in visualizing quantum circuits?\n",
        "\n",
        "3. Explain the role of the `Estimator` and `Sampler` primitives in Qiskit. How do they differ in circuit evaluation?\n",
        "\n",
        "4. What does the `EstimatorQNN.forward()` method compute, and what are its typical inputs and outputs?\n",
        "\n",
        "5. How do you enable input gradient computation in `EstimatorQNN` or `SamplerQNN`?\n",
        "\n",
        "6. What is the significance of the `callback` argument in optimizers such as COBYLA, and how is it implemented in the notebook?\n",
        "\n",
        "7. Describe the usage of `NeuralNetworkClassifier.fit()` and `NeuralNetworkClassifier.predict()`. What input and output formats do these functions expect?\n",
        "\n",
        "8. How do you interpret the output of the `NeuralNetworkRegressor.score()` method? What metric does it typically return?\n",
        "\n",
        "9. What is the difference between setting `one_hot=True` or `False` when generating datasets with `ad_hoc_data()`?\n",
        "\n",
        "10. Explain the syntax and purpose of the lambda function\n",
        "`parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2` used in the notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVvG5E-n6FHx"
      },
      "source": [
        "---\n",
        "\n",
        "# Answers to Question Set 2\n",
        "### Write here:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjcI479p78td"
      },
      "source": [
        "## Questions 3\n",
        "\n",
        "1. Is `RealAmplitudes` a parameterized ansatz used for training quantum circuits?\n",
        "2. Can `NeuralNetworkClassifier.predict()` output multi-class predictions?\n",
        "3. Does the `StatevectorEstimator` primitive provide deterministic circuit evaluation results?\n",
        "4. Is the `COBYLA` optimizer gradient-free?\n",
        "5. Does the `EstimatorQNN` require you to specify input and weight parameters separately?\n",
        "6. Can quantum neural networks trained in this notebook handle both classification and regression tasks?\n",
        "7. Does the notebook use matplotlib to visualize decision boundaries and loss curves?\n",
        "8. Is it necessary to explicitly define the number of qubits when using `QNNCircuit`?\n",
        "9. Does the `L_BFGS_B` optimizer require fewer iterations compared to COBYLA for convergence?\n",
        "10. Are callback functions optional when training quantum neural networks in Qiskit?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEG9nexF8WC6"
      },
      "source": [
        "---\n",
        "\n",
        "## Answers to Questions 3\n",
        "\n",
        "### Write here\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVNJtHF88bFx"
      },
      "source": [
        "## Questions 4\n",
        "\n",
        "1. Which Qiskit class represents a parameterized quantum circuit?\n",
        "2. Name the function used to visualize quantum circuits in matplotlib.\n",
        "3. What backend primitive is used for noiseless expectation value computation?\n",
        "4. Which optimizer is gradient-free and used for classification?\n",
        "5. What method trains the `NeuralNetworkClassifier`?\n",
        "6. Which loss function is used for regression in the notebook?\n",
        "7. What dataset function generates the ad hoc classification data?\n",
        "8. Which Python package is used for plotting?\n",
        "9. What output format does `NeuralNetworkClassifier.predict()` return?\n",
        "10. What gate does `ZZFeatureMap` primarily use for entanglement?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBycUKse8rZI"
      },
      "source": [
        "---\n",
        "\n",
        "## Answers to Questions 4\n",
        "\n",
        "### Write here\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBtzyNb68tP1"
      },
      "source": [
        "## Questions 5\n",
        "\n",
        "1. What method evaluates the QNN’s prediction?\n",
        "2. Which Qiskit class wraps feature map and ansatz?\n",
        "3. Name the function to generate random datasets.\n",
        "4. What encoding gate is used in `ZZFeatureMap`?\n",
        "5. What Python keyword defines anonymous functions?\n",
        "6. Which function plots training loss dynamically?\n",
        "7. What is the typical input type for `NeuralNetworkRegressor.fit()`?\n",
        "8. Which module provides classical optimizers like L\\_BFGS\\_B?\n",
        "9. What is the default measurement basis for `EstimatorQNN`?\n",
        "10. Which Qiskit module contains the `Sampler` primitive?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3MkfqG18y5f"
      },
      "source": [
        "---\n",
        "\n",
        "## Answers to Questions 5\n",
        "\n",
        "### Write here\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
